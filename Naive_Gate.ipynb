{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e492e3b4-7826-47e4-a08b-c0931cfbbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, input_size, output_size):\n",
    "        super(Expert, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.output_projection(x)\n",
    "\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts, d_model, nhead, num_layers, input_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(d_model, nhead, num_layers, input_size, output_size) for _ in range(num_experts)])\n",
    "        self.gates = nn.Linear(input_size, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = F.softmax(self.gates(x), dim=1)\n",
    "        outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
    "        return (weights.unsqueeze(2) * outputs).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104fdbc2-8cd9-4809-b8cb-18309b761fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shijimao/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred shape: torch.Size([100, 1])\n",
      "Target shape: torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "### Naive data simulation and test\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "model = MixtureOfExperts(num_experts=10, d_model=512, nhead=8, num_layers=6, input_size=512, output_size=512)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "# Generate input features x\n",
    "x = torch.randn(n_samples, input_dim)\n",
    "moe = MoE(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_experts=4,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "# Generate target y_1 as a nonlinear function of x\n",
    "# (e.g., combination of sin, square, and linear components)\n",
    "with torch.no_grad():\n",
    "    y_1 = (\n",
    "        torch.sin(x[:, 0]) +\n",
    "        torch.square(x[:, 1]) +\n",
    "        0.5 * x[:, 2] +\n",
    "        0.1 * torch.randn(n_samples)  # noise\n",
    "    ).unsqueeze(1)  # Shape: [n_samples, 1]\n",
    "\n",
    "\n",
    "# Wrap x and y_1 into a dataset\n",
    "dataset = TensorDataset(x, y_1)\n",
    "\n",
    "# Use DataLoader for batching\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "y_pred = moe(x)\n",
    "\n",
    "print(\"Pred shape:\", y_pred.shape)\n",
    "print(\"Target shape:\", y_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442abdfd-6f85-48a7-89e5-963a7c57be5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690395ab-db36-4c2b-854d-1976c1047e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 251.8305\n",
      "Epoch 2/50, Loss: 7.7350\n",
      "Epoch 3/50, Loss: 5.2233\n",
      "Epoch 4/50, Loss: 4.6946\n",
      "Epoch 5/50, Loss: 4.4399\n",
      "Epoch 6/50, Loss: 4.1741\n",
      "Epoch 7/50, Loss: 4.0037\n",
      "Epoch 8/50, Loss: 4.3245\n",
      "Epoch 9/50, Loss: 4.3595\n",
      "Epoch 10/50, Loss: 3.9858\n",
      "Epoch 11/50, Loss: 4.1253\n",
      "Epoch 12/50, Loss: 4.0904\n",
      "Epoch 13/50, Loss: 4.5884\n",
      "Epoch 14/50, Loss: 5.2132\n",
      "Epoch 15/50, Loss: 5.1471\n",
      "Epoch 16/50, Loss: 4.3960\n",
      "Epoch 17/50, Loss: 4.3824\n",
      "Epoch 18/50, Loss: 4.2401\n",
      "Epoch 19/50, Loss: 4.0166\n",
      "Epoch 20/50, Loss: 4.2433\n",
      "Epoch 21/50, Loss: 5.0364\n",
      "Epoch 22/50, Loss: 5.1377\n",
      "Epoch 23/50, Loss: 4.2895\n",
      "Epoch 24/50, Loss: 4.1380\n",
      "Epoch 25/50, Loss: 3.9719\n",
      "Epoch 26/50, Loss: 4.0614\n",
      "Epoch 27/50, Loss: 4.0652\n",
      "Epoch 28/50, Loss: 3.9310\n",
      "Epoch 29/50, Loss: 4.1068\n",
      "Epoch 30/50, Loss: 4.5606\n",
      "Epoch 31/50, Loss: 4.0467\n",
      "Epoch 32/50, Loss: 4.0165\n",
      "Epoch 33/50, Loss: 4.1530\n",
      "Epoch 34/50, Loss: 4.2464\n",
      "Epoch 35/50, Loss: 4.9127\n",
      "Epoch 36/50, Loss: 4.1884\n",
      "Epoch 37/50, Loss: 4.1163\n",
      "Epoch 38/50, Loss: 3.9597\n",
      "Epoch 39/50, Loss: 4.1858\n",
      "Epoch 40/50, Loss: 4.2533\n",
      "Epoch 41/50, Loss: 5.1136\n",
      "Epoch 42/50, Loss: 4.9463\n",
      "Epoch 43/50, Loss: 3.9969\n",
      "Epoch 44/50, Loss: 4.0913\n",
      "Epoch 45/50, Loss: 4.0752\n",
      "Epoch 46/50, Loss: 4.2237\n",
      "Epoch 47/50, Loss: 4.1945\n",
      "Epoch 48/50, Loss: 4.3006\n",
      "Epoch 49/50, Loss: 4.4972\n",
      "Epoch 50/50, Loss: 4.4524\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(moe.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    moe.train()\n",
    "\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = moe(batch_x)\n",
    "        loss = loss_fn(pred_y, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898cd2b3-3da5-42ca-9922-d10622f2b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, input_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.output_projection = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer_encoder(x.unsqueeze(0)).squeeze(0)  # Transformer expects (S, N, E), adjusting for N=1\n",
    "        return self.output_projection(x)\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.layer(x), dim=-1)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, d_model, nhead, num_layers, top_k=2):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([Expert(d_model, nhead, num_layers, input_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.router = Router(input_dim, num_experts)\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        routing_weights = self.router(x)\n",
    "        \n",
    "        topk_vals, topk_indices = torch.topk(routing_weights, self.top_k, dim=1)\n",
    "        topk_vals_normalized = topk_vals / topk_vals.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, output_dim, device=x.device)\n",
    "        \n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = (topk_indices == i).float()\n",
    "            if expert_mask.any():\n",
    "                expert_mask = expert_mask.unsqueeze(-1).expand(-1, -1, x.size(1))\n",
    "                inputs_to_expert = x.unsqueeze(1).repeat(1, self.top_k, 1) * expert_mask\n",
    "                inputs_to_expert = inputs_to_expert.view(-1, x.size(1))\n",
    "                expert_outputs = expert(inputs_to_expert).view(batch_size, self.top_k, -1)\n",
    "                \n",
    "                # Weight outputs by normalized routing probability and sum across selected experts\n",
    "                weighted_expert_outputs = expert_outputs * topk_vals_normalized.unsqueeze(-1)\n",
    "                outputs += weighted_expert_outputs.sum(dim=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a388bdc9-de97-421e-95a8-400d682fc933",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MoE.__init__() missing 1 required positional argument: 'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m user_cate_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_list\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_num\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)}\n\u001b[1;32m      8\u001b[0m item_cate_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_cate\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_num\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)}\n\u001b[0;32m----> 9\u001b[0m mmoe \u001b[38;5;241m=\u001b[39m MoE(user_cate_dict, item_cate_dict)\n\u001b[1;32m     10\u001b[0m outs \u001b[38;5;241m=\u001b[39m mmoe(a)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(outs)\n",
      "\u001b[0;31mTypeError\u001b[0m: MoE.__init__() missing 1 required positional argument: 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.from_numpy(np.array([[1, 2, 4, 2, 0.5, 0.1],\n",
    "                                   [4, 5, 3, 8, 0.6, 0.43],\n",
    "                                   [6, 3, 2, 9, 0.12, 0.32],\n",
    "                                   [9, 1, 1, 1, 0.12, 0.45],\n",
    "                                   [8, 3, 1, 4, 0.21, 0.67]]))\n",
    "user_cate_dict = {'user_id': (11, 0), 'user_list': (12, 3), 'user_num': (1, 4)}\n",
    "item_cate_dict = {'item_id': (8, 1), 'item_cate': (6, 2), 'item_num': (1, 5)}\n",
    "mmoe = MoE(user_cate_dict, item_cate_dict,number)\n",
    "outs = mmoe(a)\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb564dc-ecf2-4c8e-9ff4-749806e9d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FFNExpert(nn.Module):\n",
    "    \"\"\"An FFN expert similar to the FFN in Transformer Encoder Layers.\"\"\"\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super(FFNExpert, self).__init__()\n",
    "        self.input_projection = nn.Linear(d_model, hidden_dim)\n",
    "        self.output_projection = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"Router to distribute tokens to experts.\"\"\"\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super(Router, self).__init__()\n",
    "        self.layer = nn.Linear(d_model, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.layer(x), dim=-1)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, d_model, num_experts, hidden_dim, top_k=2):\n",
    "        super(MoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([FFNExpert(d_model, hidden_dim) for _ in range(num_experts)])\n",
    "        self.router = Router(d_model, num_experts)\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten to (batch_size*seq_len, d_model) for the router\n",
    "        routing_weights = self.router(x)\n",
    "        topk_vals, topk_indices = torch.topk(routing_weights, self.top_k, dim=1)\n",
    "        topk_vals_normalized = topk_vals / topk_vals.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        output = torch.zeros_like(x)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = (topk_indices == i).float()\n",
    "            if expert_mask.any():\n",
    "                inputs_to_expert = x * expert_mask.unsqueeze(-1)\n",
    "                expert_output = expert(inputs_to_expert)\n",
    "                output += expert_output * topk_vals_normalized.unsqueeze(-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayerWithMoE(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_experts, hidden_dim, dropout=0.1, top_k=2):\n",
    "        super(TransformerEncoderLayerWithMoE, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.moe = MoE(d_model, num_experts, hidden_dim, top_k=top_k)\n",
    "        # Implementation of Feedforward model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Utilize mixture of experts \n",
    "        src2 = self.moe(src)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2b8691-671b-4b08-b088-d77343c83450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulation.data_simulation_base import simulate_JM_base\n",
    "n_sim = 1\n",
    "I = 1000\n",
    "obstime = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "landmark_times = [1,2,3,4,5]\n",
    "pred_windows = [1,2,3]\n",
    "scenario = \"none\" # [\"none\", \"interaction\", \"nonph\"]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_simulation_base import simulate_JM_base\n",
    "data_all = simulate_JM_base(I=I, obstime=obstime, opt=scenario, seed=n_sim)\n",
    "data = data_all[data_all.obstime <= data_all.time]\n",
    "\n",
    "## split train/test\n",
    "random_id = range(I) #np.random.permutation(range(I))\n",
    "train_id = random_id[0:int(0.7*I)]\n",
    "test_id = random_id[int(0.7*I):I]\n",
    "\n",
    "train_data = data[data[\"id\"].isin(train_id)]\n",
    "test_data = data[data[\"id\"].isin(test_id)]\n",
    "x1= train_data[['X1','X2']]\n",
    "y = train_data[['Y1','Y2','Y3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f8566f4-36f2-4306-aee6-0930b8d6300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visit</th>\n",
       "      <th>obstime</th>\n",
       "      <th>predtime</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>pred_Y1</th>\n",
       "      <th>pred_Y2</th>\n",
       "      <th>pred_Y3</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>11.943728</td>\n",
       "      <td>-3.032593</td>\n",
       "      <td>2.760192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>11.943728</td>\n",
       "      <td>-3.032593</td>\n",
       "      <td>2.760192</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>12.255357</td>\n",
       "      <td>-5.431790</td>\n",
       "      <td>4.225383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>12.255357</td>\n",
       "      <td>-5.431790</td>\n",
       "      <td>4.225383</td>\n",
       "      <td>0.999397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>12.491947</td>\n",
       "      <td>-6.953460</td>\n",
       "      <td>2.854653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>12.491947</td>\n",
       "      <td>-6.953460</td>\n",
       "      <td>2.854653</td>\n",
       "      <td>0.998135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>16.406431</td>\n",
       "      <td>-8.508030</td>\n",
       "      <td>4.766191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>16.406431</td>\n",
       "      <td>-8.508030</td>\n",
       "      <td>4.766191</td>\n",
       "      <td>0.995494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>16.632347</td>\n",
       "      <td>-9.813989</td>\n",
       "      <td>5.816555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>16.632347</td>\n",
       "      <td>-9.813989</td>\n",
       "      <td>5.816555</td>\n",
       "      <td>0.989983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  visit  obstime  predtime  time  event         Y1        Y2        Y3  \\\n",
       "0   0      0        0         0     9   True  11.943728 -3.032593  2.760192   \n",
       "1   0      1        1         1     9   True  12.255357 -5.431790  4.225383   \n",
       "2   0      2        2         2     9   True  12.491947 -6.953460  2.854653   \n",
       "3   0      3        3         3     9   True  16.406431 -8.508030  4.766191   \n",
       "4   0      4        4         4     9   True  16.632347 -9.813989  5.816555   \n",
       "\n",
       "    X1        X2    pred_Y1   pred_Y2   pred_Y3      true  \n",
       "0  1.0  0.680195  11.943728 -3.032593  2.760192  1.000000  \n",
       "1  1.0  0.680195  12.255357 -5.431790  4.225383  0.999397  \n",
       "2  1.0  0.680195  12.491947 -6.953460  2.854653  0.998135  \n",
       "3  1.0  0.680195  16.406431 -8.508030  4.766191  0.995494  \n",
       "4  1.0  0.680195  16.632347 -9.813989  5.816555  0.989983  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5488c81b-0dd9-4c89-bcee-ac709174f076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c7b54328690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"/home/shijimao/TransformerJM/Models\")\n",
    "sys.path.append(\"/home/shijimao/TransformerJM/Simulation\")\n",
    "from Models.Transformer.functions import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "from Models.Transformer.loss import (long_loss, surv_loss)\n",
    "from Models.metrics import (AUC, Brier, MSE)\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f198d-1429-4748-accd-ee1ff8a1a658",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m train_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(train_id)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_id), batch_size):\n\u001b[0;32m---> 22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m     indices \u001b[38;5;241m=\u001b[39m train_id[batch:batch\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     25\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m train_data[train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(indices)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "from datetime import datetime\n",
    "\n",
    "n_epoch = 50\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda')\n",
    "        batch_long_out = batch_long[:,1:,:].to('cuda')\n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        #loss = loss1 + loss2\n",
    "        loss = multi_task_loss(loss1, loss2)\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "        loss1_list.append(loss1.tolist())\n",
    "        loss2_list.append(loss2.tolist())\n",
    "    loss_values.append(running_loss.tolist())\n",
    "plt.plot((loss_values-np.min(loss_values))/(np.max(loss_values)-np.min(loss_values)), 'b-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146baa36-9d58-4b3f-959e-17f5a7fa14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_long:\n",
    "        Number of longitudinal outcomes\n",
    "    d_base:\n",
    "        Number of baseline / time-independent covariates\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_long,\n",
    "                 d_base,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 num_decoder_layers,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(d_long + d_base, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model)\n",
    "            )\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(d_model,nhead,dropout)\n",
    "                                             for _ in range(num_decoder_layers)])\n",
    "        \n",
    "    def forward(self, long, base, mask, obs_time):\n",
    "        # Concatenate longitudinal and baseline data\n",
    "        x = torch.cat((long, base), dim=2)\n",
    "        \n",
    "        # Linear Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Positional Embedding\n",
    "\n",
    "        x = x + positional_encoding(\n",
    "            x.shape[0], x.shape[1], x.shape[2], obs_time)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        for layer in self.decoder_layers:\n",
    "            decoding = layer(x, x, mask)\n",
    "\n",
    "        return decoding\n",
    "\n",
    "\n",
    "class Decoder_p(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters_\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 num_decoder_layers,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(d_model,nhead,dropout)\n",
    "                                             for _ in range(num_decoder_layers)])\n",
    "        \n",
    "    def forward(self, q, kv, mask, pred_time):\n",
    "        # Positional Embedding\n",
    "        \n",
    "        q = q + positional_encoding(\n",
    "            q.shape[0], q.shape[1], q.shape[2], pred_time)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(q, kv,mask)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer1(nn.Module):\n",
    "    \"\"\"\n",
    "    An adaptation of the transformer model (Attention is All you Need)\n",
    "    for survival analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_long:\n",
    "        Number of longitudinal outcomes\n",
    "    d_base:\n",
    "        Number of baseline / time-independent covariates\n",
    "    d_model:\n",
    "        Dimension of the input vector (post embedding)\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_long,\n",
    "                 d_base,\n",
    "                 d_model = 32,\n",
    "                 nhead = 4,\n",
    "                 n_expert = 4,\n",
    "                 d_ff = 64,  \n",
    "                 num_decoder_layers = 3,\n",
    "                 dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.decoder = Decoder(d_long, d_base, d_model, nhead, num_decoder_layers, dropout)\n",
    "\n",
    "        self.decoder_pred = Decoder_p(d_model, nhead, 1, dropout)\n",
    "        \n",
    "        self.long = nn.Sequential(\n",
    "            nn.Linear(d_model, d_long)\n",
    "        )\n",
    "        \n",
    "        self.surv = nn.Sequential(\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.mmoe_head = MMoEHead(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            n_expert=n_expert,\n",
    "            d_long=d_long\n",
    "        )\n",
    "\n",
    "    def forward(self, long, base, mask, obs_time, pred_time, use_moe = True):        \n",
    "        # Decoder Layers\n",
    "        x = self.decoder(long, base, mask, obs_time)\n",
    "        \n",
    "        # Decoder Layer with prediction time embedding\n",
    "        \n",
    "        x = self.decoder_pred(x, x, mask, pred_time)\n",
    "\n",
    "        if use_moe:\n",
    "            long, surv = self.mmoe_head(x)\n",
    "        else:\n",
    "            long = self.long(x)\n",
    "            surv = torch.sigmoid(self.surv(x))\n",
    "        return long, surv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6d018c2-690f-4bdf-88cc-5963e471656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c79a79f6810>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = Transformer1(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=7)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "n_epoch = 50\n",
    "batch_size = 32\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")    \n",
    "    \n",
    "loss_values = []\n",
    "loss_test = []\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda')\n",
    "        batch_long_out = batch_long[:,1:,:].to('cuda')\n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time = obs_time[:,:-1].to('cuda'), pred_time = obs_time[:,1:].to('cuda'), use_moe = True)\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "        loss1_list.append(loss1.tolist())\n",
    "        loss2_list.append(loss2.tolist())\n",
    "    loss_values.append(running_loss.tolist())\n",
    "plt.plot((loss_values-np.min(loss_values))/(np.max(loss_values)-np.min(loss_values)), 'b-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0127fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92.870361328125,\n",
       " 99.701904296875,\n",
       " 104.06608581542969,\n",
       " 85.30158233642578,\n",
       " 103.7540283203125,\n",
       " 90.94770050048828,\n",
       " 92.91517639160156,\n",
       " 93.96625518798828,\n",
       " 93.48040771484375,\n",
       " 94.89631652832031,\n",
       " 100.34090423583984,\n",
       " 104.72716522216797,\n",
       " 91.65260314941406,\n",
       " 94.87616729736328,\n",
       " 95.83678436279297,\n",
       " 93.13465881347656,\n",
       " 90.35530090332031,\n",
       " 92.65097045898438,\n",
       " 82.51585388183594,\n",
       " 88.56510162353516,\n",
       " 83.34677124023438,\n",
       " 81.6633071899414,\n",
       " 74.19544219970703,\n",
       " 75.52436065673828,\n",
       " 81.71791076660156,\n",
       " 71.40599822998047,\n",
       " 74.99201202392578,\n",
       " 72.79692077636719,\n",
       " 69.77252960205078,\n",
       " 66.197021484375,\n",
       " 65.3902816772461,\n",
       " 61.61858367919922,\n",
       " 57.623626708984375,\n",
       " 55.055824279785156,\n",
       " 59.2623176574707,\n",
       " 51.28711700439453,\n",
       " 57.5313835144043,\n",
       " 47.893863677978516,\n",
       " 47.83053970336914,\n",
       " 41.297061920166016,\n",
       " 49.72816848754883,\n",
       " 44.456398010253906,\n",
       " 33.840301513671875,\n",
       " 38.590972900390625,\n",
       " 37.576900482177734,\n",
       " 29.958927154541016,\n",
       " 27.915145874023438,\n",
       " 26.27850341796875,\n",
       " 21.441652297973633,\n",
       " 27.136526107788086,\n",
       " 27.58018684387207,\n",
       " 19.28329849243164,\n",
       " 18.069408416748047,\n",
       " 18.088180541992188,\n",
       " 12.762659072875977,\n",
       " 13.424263954162598,\n",
       " 14.09088134765625,\n",
       " 12.177013397216797,\n",
       " 9.073748588562012,\n",
       " 9.998188972473145,\n",
       " 10.095535278320312,\n",
       " 11.143566131591797,\n",
       " 11.085536003112793,\n",
       " 10.96900463104248,\n",
       " 10.803979873657227,\n",
       " 11.571563720703125,\n",
       " 11.851431846618652,\n",
       " 11.649264335632324,\n",
       " 10.654926300048828,\n",
       " 11.10879135131836,\n",
       " 11.212366104125977,\n",
       " 9.62748908996582,\n",
       " 11.677084922790527,\n",
       " 9.490438461303711,\n",
       " 10.414680480957031,\n",
       " 8.279962539672852,\n",
       " 10.204909324645996,\n",
       " 10.28165054321289,\n",
       " 9.545879364013672,\n",
       " 9.296343803405762,\n",
       " 7.209412574768066,\n",
       " 8.863239288330078,\n",
       " 8.21273422241211,\n",
       " 8.6658935546875,\n",
       " 7.544443130493164,\n",
       " 7.290195941925049,\n",
       " 7.229565143585205,\n",
       " 7.9666314125061035,\n",
       " 6.933189392089844,\n",
       " 8.986593246459961,\n",
       " 6.725815773010254,\n",
       " 6.883524417877197,\n",
       " 6.462886333465576,\n",
       " 7.308488368988037,\n",
       " 6.285016059875488,\n",
       " 5.483211517333984,\n",
       " 7.573461055755615,\n",
       " 6.527215003967285,\n",
       " 7.453965187072754,\n",
       " 5.647946834564209,\n",
       " 4.553058624267578,\n",
       " 5.538893699645996,\n",
       " 4.027453422546387,\n",
       " 5.716444492340088,\n",
       " 4.383479118347168,\n",
       " 4.291325569152832,\n",
       " 4.674720764160156,\n",
       " 4.220064640045166,\n",
       " 5.166201591491699,\n",
       " 4.922821521759033,\n",
       " 3.670079469680786,\n",
       " 5.353110313415527,\n",
       " 4.364470958709717,\n",
       " 4.746667861938477,\n",
       " 4.763565540313721,\n",
       " 4.647209644317627,\n",
       " 4.547728538513184,\n",
       " 5.040806293487549,\n",
       " 4.494900226593018,\n",
       " 5.196686744689941,\n",
       " 4.462611198425293,\n",
       " 3.3598172664642334,\n",
       " 4.3943190574646,\n",
       " 4.193279266357422,\n",
       " 4.116128921508789,\n",
       " 3.427696943283081,\n",
       " 3.5377228260040283,\n",
       " 3.354668378829956,\n",
       " 3.4578399658203125,\n",
       " 3.7419545650482178,\n",
       " 3.8150882720947266,\n",
       " 3.2798759937286377,\n",
       " 4.0774359703063965,\n",
       " 3.3616907596588135,\n",
       " 3.7184526920318604,\n",
       " 3.581533432006836,\n",
       " 3.873033285140991,\n",
       " 3.512164354324341,\n",
       " 4.843613147735596,\n",
       " 3.372511625289917,\n",
       " 3.2617061138153076,\n",
       " 3.005847930908203,\n",
       " 3.18070912361145,\n",
       " 3.786874771118164,\n",
       " 3.738602638244629,\n",
       " 3.8040521144866943,\n",
       " 3.9098808765411377,\n",
       " 2.8039865493774414,\n",
       " 3.7057769298553467,\n",
       " 3.306076765060425,\n",
       " 3.796762704849243,\n",
       " 3.8432679176330566,\n",
       " 2.991025686264038,\n",
       " 4.279523849487305,\n",
       " 3.8140945434570312,\n",
       " 3.2893755435943604,\n",
       " 2.8408279418945312,\n",
       " 3.1308693885803223,\n",
       " 3.652097225189209,\n",
       " 3.5183582305908203,\n",
       " 3.408592939376831,\n",
       " 2.6227667331695557,\n",
       " 3.124251365661621,\n",
       " 3.369250535964966,\n",
       " 3.126030921936035,\n",
       " 2.8920862674713135,\n",
       " 2.9230966567993164,\n",
       " 3.1395175457000732,\n",
       " 2.9771039485931396,\n",
       " 3.248426675796509,\n",
       " 3.030506134033203,\n",
       " 3.418870687484741,\n",
       " 4.187003135681152,\n",
       " 3.164173126220703,\n",
       " 4.248307704925537,\n",
       " 3.3820130825042725,\n",
       " 3.5415046215057373,\n",
       " 3.7360923290252686,\n",
       " 2.969356060028076,\n",
       " 3.519664764404297,\n",
       " 3.6864984035491943,\n",
       " 3.543527841567993,\n",
       " 3.731266498565674,\n",
       " 3.7288269996643066,\n",
       " 2.79046630859375,\n",
       " 2.7162539958953857,\n",
       " 3.7798385620117188,\n",
       " 3.9938600063323975,\n",
       " 3.8833529949188232,\n",
       " 2.845764398574829,\n",
       " 3.061920404434204,\n",
       " 3.749028205871582,\n",
       " 4.476977825164795,\n",
       " 2.6254796981811523,\n",
       " 3.2401371002197266,\n",
       " 3.443350315093994,\n",
       " 3.4596996307373047,\n",
       " 2.6845014095306396,\n",
       " 3.0862669944763184,\n",
       " 3.3795573711395264,\n",
       " 3.723320245742798,\n",
       " 2.957695960998535,\n",
       " 4.28869104385376,\n",
       " 2.946535110473633,\n",
       " 3.949195146560669,\n",
       " 2.974701166152954,\n",
       " 2.790415048599243,\n",
       " 2.91339373588562,\n",
       " 2.6457393169403076,\n",
       " 3.054351568222046,\n",
       " 3.0335745811462402,\n",
       " 2.8467609882354736,\n",
       " 3.170008420944214,\n",
       " 3.2799246311187744,\n",
       " 3.190809965133667,\n",
       " 2.790928363800049,\n",
       " 2.85675311088562,\n",
       " 2.4037973880767822,\n",
       " 3.2907004356384277,\n",
       " 2.821841239929199,\n",
       " 2.6397275924682617,\n",
       " 2.637089729309082,\n",
       " 2.7281148433685303,\n",
       " 2.6239702701568604,\n",
       " 3.0256972312927246,\n",
       " 2.394538402557373,\n",
       " 3.0735604763031006,\n",
       " 3.4752018451690674,\n",
       " 2.3991124629974365,\n",
       " 3.3457884788513184,\n",
       " 2.9995501041412354,\n",
       " 2.89497447013855,\n",
       " 2.874175548553467,\n",
       " 2.6082444190979004,\n",
       " 2.5500826835632324,\n",
       " 2.8021903038024902,\n",
       " 2.4152638912200928,\n",
       " 2.5624594688415527,\n",
       " 3.088867425918579,\n",
       " 2.488638162612915,\n",
       " 2.92712664604187,\n",
       " 2.436173439025879,\n",
       " 2.2646188735961914,\n",
       " 3.216954231262207,\n",
       " 2.6753132343292236,\n",
       " 2.4266257286071777,\n",
       " 2.697462558746338,\n",
       " 2.6530518531799316,\n",
       " 3.3445353507995605,\n",
       " 2.4547882080078125,\n",
       " 2.4140076637268066,\n",
       " 2.6809346675872803,\n",
       " 3.3332552909851074,\n",
       " 3.0579566955566406,\n",
       " 2.578673839569092,\n",
       " 3.2019340991973877,\n",
       " 3.077239513397217,\n",
       " 2.819636106491089,\n",
       " 3.018371105194092,\n",
       " 3.1237528324127197,\n",
       " 3.1164238452911377,\n",
       " 2.732872247695923,\n",
       " 2.563410520553589,\n",
       " 2.4692230224609375,\n",
       " 2.3567662239074707,\n",
       " 2.698481798171997,\n",
       " 2.534097194671631,\n",
       " 2.6915338039398193,\n",
       " 2.4238712787628174,\n",
       " 2.6284780502319336,\n",
       " 2.3199164867401123,\n",
       " 2.6801199913024902,\n",
       " 2.653552293777466,\n",
       " 2.687721014022827,\n",
       " 2.433553695678711,\n",
       " 2.2181079387664795,\n",
       " 2.828193426132202,\n",
       " 3.390188694000244,\n",
       " 2.861449956893921,\n",
       " 2.2966911792755127,\n",
       " 2.092348337173462,\n",
       " 2.4149787425994873,\n",
       " 2.574209213256836,\n",
       " 2.711904287338257,\n",
       " 2.7960433959960938,\n",
       " 2.3278486728668213,\n",
       " 2.2405049800872803,\n",
       " 2.4544880390167236,\n",
       " 2.4591078758239746,\n",
       " 2.309858798980713,\n",
       " 2.373260259628296,\n",
       " 2.8726325035095215,\n",
       " 2.6910128593444824,\n",
       " 2.3230435848236084,\n",
       " 2.958296060562134,\n",
       " 2.236795663833618,\n",
       " 1.9808071851730347,\n",
       " 2.392343282699585,\n",
       " 2.581313133239746,\n",
       " 2.600146532058716,\n",
       " 2.7600510120391846,\n",
       " 2.349759578704834,\n",
       " 2.430725574493408,\n",
       " 2.537576198577881,\n",
       " 2.1745643615722656,\n",
       " 2.9235479831695557,\n",
       " 2.777329683303833,\n",
       " 2.048541307449341,\n",
       " 2.7313525676727295,\n",
       " 2.5993547439575195,\n",
       " 2.337299346923828,\n",
       " 2.7594144344329834,\n",
       " 2.1180319786071777,\n",
       " 2.985090970993042,\n",
       " 2.193500518798828,\n",
       " 2.79107928276062,\n",
       " 2.1772420406341553,\n",
       " 2.5509214401245117,\n",
       " 2.4468584060668945,\n",
       " 2.410740613937378,\n",
       " 2.437117099761963,\n",
       " 2.3542354106903076,\n",
       " 3.1371066570281982,\n",
       " 2.323003053665161,\n",
       " 2.595794200897217,\n",
       " 2.298156499862671,\n",
       " 2.4525201320648193,\n",
       " 2.1937196254730225,\n",
       " 2.383096933364868,\n",
       " 2.2865657806396484,\n",
       " 2.284073829650879,\n",
       " 2.601945638656616,\n",
       " 1.9348036050796509,\n",
       " 2.3321468830108643,\n",
       " 2.565434217453003,\n",
       " 2.206123113632202,\n",
       " 2.535074472427368,\n",
       " 2.261688470840454,\n",
       " 2.5156936645507812,\n",
       " 2.283353805541992,\n",
       " 2.4988155364990234,\n",
       " 2.4560115337371826,\n",
       " 2.0417094230651855,\n",
       " 2.254871368408203,\n",
       " 2.336496353149414,\n",
       " 2.632782220840454,\n",
       " 2.69149112701416,\n",
       " 2.30130672454834,\n",
       " 2.57250714302063,\n",
       " 2.3927900791168213,\n",
       " 2.1847245693206787,\n",
       " 2.374600410461426,\n",
       " 2.2889959812164307,\n",
       " 2.279419422149658,\n",
       " 2.1251211166381836,\n",
       " 2.050126075744629,\n",
       " 2.204857349395752,\n",
       " 2.3823349475860596,\n",
       " 2.2721166610717773,\n",
       " 2.057183265686035,\n",
       " 2.443521022796631,\n",
       " 2.1978278160095215,\n",
       " 2.3025054931640625,\n",
       " 1.923009991645813,\n",
       " 2.377777338027954,\n",
       " 2.432042360305786,\n",
       " 2.546724557876587,\n",
       " 1.9157803058624268,\n",
       " 2.1835272312164307,\n",
       " 2.420163869857788,\n",
       " 2.3762471675872803,\n",
       " 2.1887900829315186,\n",
       " 2.804919719696045,\n",
       " 2.2323296070098877,\n",
       " 2.020815849304199,\n",
       " 2.3850462436676025,\n",
       " 2.044149398803711,\n",
       " 2.7428367137908936,\n",
       " 2.078219413757324,\n",
       " 1.8433438539505005,\n",
       " 2.134002923965454,\n",
       " 1.9959092140197754,\n",
       " 2.787811279296875,\n",
       " 2.10551118850708,\n",
       " 2.185764789581299,\n",
       " 2.342590808868408,\n",
       " 2.794865608215332,\n",
       " 2.3479936122894287,\n",
       " 2.3134407997131348,\n",
       " 2.18609356880188,\n",
       " 2.1541950702667236,\n",
       " 2.2232484817504883,\n",
       " 1.8644121885299683,\n",
       " 1.9804904460906982,\n",
       " 1.8055121898651123,\n",
       " 2.3961610794067383,\n",
       " 2.303853988647461,\n",
       " 2.2288191318511963,\n",
       " 2.2202579975128174,\n",
       " 1.9932081699371338,\n",
       " 2.1352338790893555,\n",
       " 2.1312992572784424,\n",
       " 2.478383779525757,\n",
       " 1.8692327737808228,\n",
       " 2.3811752796173096,\n",
       " 2.817662000656128,\n",
       " 2.276350975036621,\n",
       " 2.1375277042388916,\n",
       " 1.8704437017440796,\n",
       " 2.0770719051361084,\n",
       " 2.027493715286255,\n",
       " 2.0498082637786865,\n",
       " 1.974596619606018,\n",
       " 1.9618415832519531,\n",
       " 1.7913970947265625,\n",
       " 2.465313673019409,\n",
       " 2.1540191173553467,\n",
       " 2.0557005405426025,\n",
       " 2.0947084426879883,\n",
       " 2.3980021476745605,\n",
       " 2.433490037918091,\n",
       " 1.9648343324661255,\n",
       " 2.44111967086792,\n",
       " 2.1556525230407715,\n",
       " 2.0120368003845215,\n",
       " 1.8214565515518188,\n",
       " 2.4389147758483887,\n",
       " 2.368593692779541,\n",
       " 2.0311973094940186,\n",
       " 1.7843985557556152,\n",
       " 2.1997227668762207,\n",
       " 2.0619213581085205,\n",
       " 2.20414137840271,\n",
       " 2.058319568634033,\n",
       " 1.987176537513733,\n",
       " 2.03702712059021,\n",
       " 1.6485755443572998,\n",
       " 2.082505226135254,\n",
       " 2.0171618461608887,\n",
       " 2.1675262451171875,\n",
       " 1.6777949333190918,\n",
       " 2.010399103164673,\n",
       " 1.7601646184921265,\n",
       " 1.864074945449829,\n",
       " 1.73855721950531,\n",
       " 2.183562755584717,\n",
       " 1.9697133302688599,\n",
       " 1.954997181892395,\n",
       " 2.0250790119171143,\n",
       " 1.9878381490707397,\n",
       " 2.026813507080078,\n",
       " 1.894478440284729,\n",
       " 2.0659613609313965,\n",
       " 1.918189525604248,\n",
       " 2.4433000087738037,\n",
       " 2.0597712993621826,\n",
       " 2.0227882862091064,\n",
       " 2.1435904502868652,\n",
       " 1.8944462537765503,\n",
       " 1.9787760972976685,\n",
       " 2.1579673290252686,\n",
       " 2.177729606628418,\n",
       " 1.9612864255905151,\n",
       " 2.298292398452759,\n",
       " 1.8277311325073242,\n",
       " 2.201500415802002,\n",
       " 1.9758901596069336,\n",
       " 2.1293232440948486,\n",
       " 1.860888123512268,\n",
       " 2.258361577987671,\n",
       " 1.9490973949432373,\n",
       " 1.793388843536377,\n",
       " 2.2436203956604004,\n",
       " 2.0663962364196777,\n",
       " 1.8306455612182617,\n",
       " 2.230365514755249,\n",
       " 1.8852283954620361,\n",
       " 2.033846616744995,\n",
       " 2.1621344089508057,\n",
       " 2.105177879333496,\n",
       " 1.9827914237976074,\n",
       " 2.0754055976867676,\n",
       " 2.1042566299438477,\n",
       " 2.402009963989258,\n",
       " 2.0287930965423584,\n",
       " 2.4087374210357666,\n",
       " 2.041934013366699,\n",
       " 1.9558532238006592,\n",
       " 1.874826192855835,\n",
       " 2.1929514408111572,\n",
       " 1.9541113376617432,\n",
       " 1.9396369457244873,\n",
       " 2.0399532318115234,\n",
       " 2.1683058738708496,\n",
       " 2.229430675506592,\n",
       " 2.0322041511535645,\n",
       " 1.9254480600357056,\n",
       " 1.9866489171981812,\n",
       " 2.1720173358917236,\n",
       " 1.9827712774276733,\n",
       " 2.016042470932007,\n",
       " 2.0445845127105713,\n",
       " 1.945695161819458,\n",
       " 2.320789337158203,\n",
       " 2.120176315307617,\n",
       " 2.0573110580444336,\n",
       " 1.8481954336166382,\n",
       " 2.46740460395813,\n",
       " 1.8952964544296265,\n",
       " 2.0215110778808594,\n",
       " 1.810850739479065,\n",
       " 2.601170301437378,\n",
       " 2.103743076324463,\n",
       " 2.0006630420684814,\n",
       " 1.9694304466247559,\n",
       " 2.6247658729553223,\n",
       " 2.1181106567382812,\n",
       " 2.0842199325561523,\n",
       " 2.0347838401794434,\n",
       " 1.820487380027771,\n",
       " 2.1112730503082275,\n",
       " 2.517352819442749,\n",
       " 1.887436866760254,\n",
       " 1.8546572923660278,\n",
       " 2.0478219985961914,\n",
       " 1.9046419858932495,\n",
       " 2.2301814556121826,\n",
       " 1.8340657949447632,\n",
       " 1.7795748710632324,\n",
       " 1.9114125967025757,\n",
       " 2.0199265480041504,\n",
       " 1.9359747171401978,\n",
       " 1.966827630996704,\n",
       " 1.9302595853805542,\n",
       " 1.761818289756775,\n",
       " 2.016347885131836,\n",
       " 2.011399745941162,\n",
       " 1.9783722162246704,\n",
       " 1.6154558658599854,\n",
       " 1.6765624284744263,\n",
       " 1.919722318649292,\n",
       " 1.9033901691436768,\n",
       " 1.9776604175567627,\n",
       " 2.005044937133789,\n",
       " 1.8612183332443237,\n",
       " 1.8354504108428955,\n",
       " 2.1105403900146484,\n",
       " 1.8887454271316528,\n",
       " 2.0153300762176514,\n",
       " 1.7575352191925049,\n",
       " 1.8263330459594727,\n",
       " 1.8257750272750854,\n",
       " 2.104949712753296,\n",
       " 1.88648521900177,\n",
       " 1.6536904573440552,\n",
       " 1.9005718231201172,\n",
       " 1.7443134784698486,\n",
       " 2.1115753650665283,\n",
       " 1.9726811647415161,\n",
       " 2.0630605220794678,\n",
       " 1.9885146617889404,\n",
       " 2.130553722381592,\n",
       " 1.7638230323791504,\n",
       " 2.049198627471924,\n",
       " 1.9407953023910522,\n",
       " 1.9035288095474243,\n",
       " 1.9658132791519165,\n",
       " 1.9196354150772095,\n",
       " 1.6775540113449097,\n",
       " 1.9384256601333618,\n",
       " 1.760472297668457,\n",
       " 1.8833807706832886,\n",
       " 1.9169424772262573,\n",
       " 1.7227169275283813,\n",
       " 1.856315016746521,\n",
       " 1.8655256032943726,\n",
       " 1.8936041593551636,\n",
       " 1.6908824443817139,\n",
       " 1.8358824253082275,\n",
       " 1.666237711906433,\n",
       " 1.7607847452163696,\n",
       " 1.8415956497192383,\n",
       " 2.2828266620635986,\n",
       " 2.1199724674224854,\n",
       " 1.9357585906982422,\n",
       " 1.6701899766921997,\n",
       " 2.0240724086761475,\n",
       " 1.786049723625183,\n",
       " 1.6085426807403564,\n",
       " 1.9082648754119873,\n",
       " 2.137835741043091,\n",
       " 1.8614543676376343,\n",
       " 1.865140438079834,\n",
       " 2.0113089084625244,\n",
       " 1.7972009181976318,\n",
       " 1.9317941665649414,\n",
       " 1.6775163412094116,\n",
       " 2.076748847961426,\n",
       " 1.8464934825897217,\n",
       " 1.8880958557128906,\n",
       " 1.682305097579956,\n",
       " 1.7524662017822266,\n",
       " 1.6839072704315186,\n",
       " 1.9171072244644165,\n",
       " 1.7500524520874023,\n",
       " 1.8257635831832886,\n",
       " 1.9095453023910522,\n",
       " 1.958123803138733,\n",
       " 2.1552414894104004,\n",
       " 1.746425747871399,\n",
       " 1.685555100440979,\n",
       " 1.8716720342636108,\n",
       " 1.728851079940796,\n",
       " 1.7196886539459229,\n",
       " 2.0412073135375977,\n",
       " 1.7500035762786865,\n",
       " 1.965972661972046,\n",
       " 1.9757345914840698,\n",
       " 1.8508412837982178,\n",
       " 1.704633355140686,\n",
       " 1.7166379690170288,\n",
       " 1.8579769134521484,\n",
       " 1.9621942043304443,\n",
       " 2.082162380218506,\n",
       " 1.881550908088684,\n",
       " 1.7426973581314087,\n",
       " 1.6813901662826538,\n",
       " 1.904937505722046,\n",
       " 2.0061228275299072,\n",
       " 2.113102436065674,\n",
       " 1.7132914066314697,\n",
       " 1.815826654434204,\n",
       " 1.8893067836761475,\n",
       " 2.1094110012054443,\n",
       " 1.6485835313796997,\n",
       " 1.8536975383758545,\n",
       " 1.9250980615615845,\n",
       " 2.3665823936462402,\n",
       " 1.6853365898132324,\n",
       " 2.0935161113739014,\n",
       " 2.2048075199127197,\n",
       " 2.2605793476104736,\n",
       " 1.8842860460281372,\n",
       " 1.9194978475570679,\n",
       " 1.8889156579971313,\n",
       " 1.8636831045150757,\n",
       " 1.9070271253585815,\n",
       " 1.9361248016357422,\n",
       " 1.6756391525268555,\n",
       " 1.8794420957565308,\n",
       " 1.8252851963043213,\n",
       " 2.166902780532837,\n",
       " 1.7882118225097656,\n",
       " 1.8943990468978882,\n",
       " 1.6897770166397095,\n",
       " 1.8191338777542114,\n",
       " 1.6412608623504639,\n",
       " 1.8204134702682495,\n",
       " 1.8863751888275146,\n",
       " 2.0315356254577637,\n",
       " 1.6786667108535767,\n",
       " 1.5735752582550049,\n",
       " 1.9477488994598389,\n",
       " 1.9039945602416992,\n",
       " 1.7032989263534546,\n",
       " 2.1414847373962402,\n",
       " 1.769982933998108,\n",
       " 1.8709497451782227,\n",
       " 1.6482534408569336,\n",
       " 1.8845449686050415,\n",
       " 1.899998426437378,\n",
       " 1.6279164552688599,\n",
       " 1.7388912439346313,\n",
       " 1.841403841972351,\n",
       " 1.9182323217391968,\n",
       " 1.5993754863739014,\n",
       " 1.6671898365020752,\n",
       " 2.0496480464935303,\n",
       " 1.6188101768493652,\n",
       " 1.9031355381011963,\n",
       " 1.9254776239395142,\n",
       " 2.113121271133423,\n",
       " 1.9284214973449707,\n",
       " 1.835771918296814,\n",
       " 1.5456348657608032,\n",
       " 1.6107122898101807,\n",
       " 1.6705681085586548,\n",
       " 1.4847537279129028,\n",
       " 2.0605623722076416,\n",
       " 1.7464019060134888,\n",
       " 1.649343490600586,\n",
       " 1.9401037693023682,\n",
       " 1.5772948265075684,\n",
       " 1.9384468793869019,\n",
       " 1.7978233098983765,\n",
       " 1.9597796201705933,\n",
       " 1.755773901939392,\n",
       " 1.6841001510620117,\n",
       " 2.000553607940674,\n",
       " 1.8307409286499023,\n",
       " 2.027268648147583,\n",
       " 1.8377975225448608,\n",
       " 1.6023300886154175,\n",
       " 1.862857699394226,\n",
       " 1.9701621532440186,\n",
       " 1.716322898864746,\n",
       " 1.6537379026412964,\n",
       " 1.7988500595092773,\n",
       " 1.7298760414123535,\n",
       " 1.729927659034729,\n",
       " 1.6815941333770752,\n",
       " 1.773758888244629,\n",
       " 1.879737377166748,\n",
       " 1.823362946510315,\n",
       " 1.714664101600647,\n",
       " 1.6403993368148804,\n",
       " 1.8302690982818604,\n",
       " 2.0002973079681396,\n",
       " 2.0232017040252686,\n",
       " 2.034904718399048,\n",
       " 1.7655974626541138,\n",
       " 1.8086344003677368,\n",
       " 1.762663722038269,\n",
       " 1.6172428131103516,\n",
       " 1.754696249961853,\n",
       " 1.791178822517395,\n",
       " 2.1357791423797607,\n",
       " 1.719653844833374,\n",
       " 1.8062130212783813,\n",
       " 1.827094316482544,\n",
       " 1.7755262851715088,\n",
       " 1.8802638053894043,\n",
       " 1.7870155572891235,\n",
       " 1.6988351345062256,\n",
       " 1.8854330778121948,\n",
       " 1.7277379035949707,\n",
       " 1.823620319366455,\n",
       " 1.7536036968231201,\n",
       " 1.8841156959533691,\n",
       " 1.8865183591842651,\n",
       " 1.8666267395019531,\n",
       " 1.572654366493225,\n",
       " 1.613853931427002,\n",
       " 1.6172411441802979,\n",
       " 1.7207497358322144,\n",
       " 1.5893207788467407,\n",
       " 1.7431082725524902,\n",
       " 2.096156358718872,\n",
       " 1.6390353441238403,\n",
       " 1.8148829936981201,\n",
       " 1.7698402404785156,\n",
       " 1.9054133892059326,\n",
       " 1.8345143795013428,\n",
       " 1.8103597164154053,\n",
       " 1.7083638906478882,\n",
       " 1.6205410957336426,\n",
       " 2.124134063720703,\n",
       " 1.8324294090270996,\n",
       " 1.6450356245040894,\n",
       " 1.815004587173462,\n",
       " 1.633354663848877,\n",
       " 1.8037402629852295,\n",
       " 1.669536828994751,\n",
       " 1.490348219871521,\n",
       " 2.0854318141937256,\n",
       " 1.530029296875,\n",
       " 1.7730510234832764,\n",
       " 2.0948500633239746,\n",
       " 1.7180284261703491,\n",
       " 1.7930793762207031,\n",
       " 1.6581132411956787,\n",
       " 1.605133295059204,\n",
       " 1.7662094831466675,\n",
       " 1.620521068572998,\n",
       " 1.7897485494613647,\n",
       " 2.0373847484588623,\n",
       " 1.8274452686309814,\n",
       " 1.8875466585159302,\n",
       " 1.6408412456512451,\n",
       " 1.8222531080245972,\n",
       " 1.6967262029647827,\n",
       " 1.7859582901000977,\n",
       " 1.839267611503601,\n",
       " 1.8798112869262695,\n",
       " 1.750543236732483,\n",
       " 1.810392141342163,\n",
       " 1.8187503814697266,\n",
       " 1.7428112030029297,\n",
       " 2.024052143096924,\n",
       " 1.7550439834594727,\n",
       " 1.8865869045257568,\n",
       " 2.0958645343780518,\n",
       " 1.5443464517593384,\n",
       " 1.4946303367614746,\n",
       " 2.0205419063568115,\n",
       " 1.785565733909607,\n",
       " 1.9978854656219482,\n",
       " 1.901634931564331,\n",
       " 1.6941674947738647,\n",
       " 1.7631502151489258,\n",
       " 2.0304746627807617,\n",
       " 1.8391286134719849,\n",
       " 2.40665602684021,\n",
       " 1.7047191858291626,\n",
       " 1.839457392692566,\n",
       " 1.7612866163253784,\n",
       " 1.7591990232467651,\n",
       " 1.5886626243591309,\n",
       " 2.0882835388183594,\n",
       " 1.7341727018356323,\n",
       " 1.7194803953170776,\n",
       " 1.8623955249786377,\n",
       " 1.664914846420288,\n",
       " 1.7201324701309204,\n",
       " 2.0481178760528564,\n",
       " 1.9006301164627075,\n",
       " 1.7985661029815674,\n",
       " 1.6725449562072754,\n",
       " 1.503281831741333,\n",
       " 1.9186689853668213,\n",
       " 1.947495460510254,\n",
       " 1.7257745265960693,\n",
       " 1.7222098112106323,\n",
       " 1.6727147102355957,\n",
       " 1.9490346908569336,\n",
       " 1.8553439378738403,\n",
       " 1.697028636932373,\n",
       " 1.6439785957336426,\n",
       " 1.5900933742523193,\n",
       " 1.6236492395401,\n",
       " 1.6698193550109863,\n",
       " 1.7380824089050293,\n",
       " 1.8689963817596436,\n",
       " 1.7298924922943115,\n",
       " 1.6927765607833862,\n",
       " 2.0297012329101562,\n",
       " 1.6277211904525757,\n",
       " 2.0009355545043945,\n",
       " 1.7420049905776978,\n",
       " 1.6235417127609253,\n",
       " 1.9852403402328491,\n",
       " 1.620526909828186,\n",
       " 1.8511381149291992,\n",
       " 1.5926637649536133,\n",
       " 1.6200079917907715,\n",
       " 1.6730726957321167,\n",
       " 1.6414343118667603,\n",
       " 1.601819634437561,\n",
       " 1.8247431516647339,\n",
       " 1.6958842277526855,\n",
       " 1.5675363540649414,\n",
       " 1.9274585247039795,\n",
       " 1.7021458148956299,\n",
       " 1.6318731307983398,\n",
       " 2.0836691856384277,\n",
       " 1.619886040687561,\n",
       " 1.7918025255203247,\n",
       " 1.8286076784133911,\n",
       " 1.8990769386291504,\n",
       " 1.7694361209869385,\n",
       " 1.6187890768051147,\n",
       " 1.7423467636108398,\n",
       " 1.4926609992980957,\n",
       " 1.7384421825408936,\n",
       " 1.6577425003051758,\n",
       " 1.9414193630218506,\n",
       " 1.8364146947860718,\n",
       " 1.6392881870269775,\n",
       " 1.6154993772506714,\n",
       " 1.5623691082000732,\n",
       " 1.524128794670105,\n",
       " 1.6402711868286133,\n",
       " 1.6596903800964355,\n",
       " 1.6676913499832153,\n",
       " 1.6600850820541382,\n",
       " 1.889542818069458,\n",
       " 1.7072174549102783,\n",
       " 1.5270344018936157,\n",
       " 1.7904192209243774,\n",
       " 1.4879049062728882,\n",
       " 2.0056068897247314,\n",
       " 1.6152244806289673,\n",
       " 1.536926031112671,\n",
       " 1.5877940654754639,\n",
       " 1.8153525590896606,\n",
       " 1.8134771585464478,\n",
       " 1.7505240440368652,\n",
       " 1.6624592542648315,\n",
       " 1.7518932819366455,\n",
       " 2.013489246368408,\n",
       " 1.7210736274719238,\n",
       " 1.6406567096710205,\n",
       " 1.5153409242630005,\n",
       " 1.8229392766952515,\n",
       " 1.715588927268982,\n",
       " 1.67203950881958,\n",
       " 1.7172645330429077,\n",
       " 1.7422109842300415,\n",
       " 1.5665180683135986,\n",
       " 1.7446705102920532,\n",
       " 1.5379151105880737,\n",
       " 1.5813394784927368,\n",
       " 1.8185516595840454,\n",
       " 1.604617953300476,\n",
       " 1.4748750925064087,\n",
       " 1.7342166900634766,\n",
       " 1.490571141242981,\n",
       " 1.6136648654937744,\n",
       " 1.9206349849700928,\n",
       " 1.5915052890777588,\n",
       " 1.7679537534713745,\n",
       " 1.8238286972045898,\n",
       " 1.8100264072418213,\n",
       " 1.799338698387146,\n",
       " 1.7178919315338135,\n",
       " 1.7069385051727295,\n",
       " 1.7661511898040771,\n",
       " 1.7931462526321411,\n",
       " 1.5432466268539429,\n",
       " 1.7696658372879028,\n",
       " 1.5276315212249756,\n",
       " 1.6608855724334717,\n",
       " 1.6394829750061035,\n",
       " 1.5959817171096802,\n",
       " 1.4985142946243286,\n",
       " 1.594959020614624,\n",
       " 1.8127003908157349,\n",
       " 1.6290137767791748,\n",
       " 1.5907297134399414,\n",
       " 1.822203278541565,\n",
       " 1.639181137084961,\n",
       " 1.734359860420227,\n",
       " 1.7222729921340942,\n",
       " 1.7221183776855469,\n",
       " 1.7858941555023193,\n",
       " 1.7304625511169434,\n",
       " 1.701749324798584,\n",
       " 1.6128078699111938,\n",
       " 1.7351127862930298,\n",
       " 1.9034501314163208,\n",
       " 1.7113244533538818,\n",
       " 1.785487413406372,\n",
       " 1.7446763515472412,\n",
       " 1.6771996021270752,\n",
       " 1.795875072479248,\n",
       " 2.0811688899993896,\n",
       " 1.785109281539917,\n",
       " 1.7891278266906738,\n",
       " 1.5649607181549072,\n",
       " 1.7489094734191895,\n",
       " 1.6150535345077515,\n",
       " 1.7809175252914429,\n",
       " 1.6086448431015015,\n",
       " 1.818750023841858,\n",
       " 1.6585118770599365,\n",
       " 1.5948364734649658,\n",
       " 1.614272117614746,\n",
       " 1.7030971050262451,\n",
       " 1.6183348894119263,\n",
       " 1.714487910270691,\n",
       " 1.7612379789352417,\n",
       " 1.710815191268921,\n",
       " 1.6848011016845703,\n",
       " 1.7021993398666382,\n",
       " 1.7526425123214722,\n",
       " 1.6631832122802734,\n",
       " 1.7907965183258057,\n",
       " 1.575928807258606,\n",
       " 1.7619976997375488,\n",
       " 1.7097058296203613,\n",
       " 1.8263543844223022,\n",
       " 1.5785013437271118,\n",
       " 1.6886765956878662,\n",
       " 1.743626356124878,\n",
       " 1.8971426486968994,\n",
       " 1.6841474771499634,\n",
       " 1.7026921510696411,\n",
       " 1.4918571710586548,\n",
       " 1.5409218072891235,\n",
       " 1.8524240255355835,\n",
       " 1.672095775604248,\n",
       " 1.9556061029434204,\n",
       " 1.5464469194412231,\n",
       " 1.4743032455444336,\n",
       " 1.709603190422058,\n",
       " 1.6710251569747925,\n",
       " 1.9073913097381592,\n",
       " 1.6344941854476929,\n",
       " 1.662122130393982,\n",
       " 1.769778847694397,\n",
       " 1.7569494247436523,\n",
       " 1.7926993370056152,\n",
       " 1.7853636741638184,\n",
       " 1.791574478149414,\n",
       " 1.6531473398208618,\n",
       " 1.7206346988677979,\n",
       " 1.6442011594772339,\n",
       " 1.9035511016845703,\n",
       " 1.708093285560608,\n",
       " 1.4228918552398682,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d5fa7a-32bf-4877-9922-863e06ba434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.Attention = MultiHeadAttention(d_model, nhead)\n",
    "                \n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(d_model,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,d_model),\n",
    "            nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
    "        self.layerNorm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q, kv, mask):\n",
    "        \n",
    "        # Attention\n",
    "        residual = q\n",
    "        x = self.Attention(query=q, key=kv, value=kv, mask = mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layerNorm1(x + residual)\n",
    "        \n",
    "        # Feed Forward\n",
    "        residual = x\n",
    "        x = self.feedForward(x)\n",
    "        x = self.layerNorm2(x + residual)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def positional_encoding(batch_size, length, d_model, obs_time):\n",
    "    \"\"\"\n",
    "    Positional Encoding for each visit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size:\n",
    "        Number of subjects in batch\n",
    "    length:\n",
    "        Number of visits\n",
    "    d_model:\n",
    "        Dimension of the model vector\n",
    "    obs_time:\n",
    "        Observed/recorded time of each visit\n",
    "    \"\"\"\n",
    "    PE = torch.zeros((batch_size, length, d_model)).to('cuda')\n",
    "    if obs_time.ndim == 0:\n",
    "        obs_time = obs_time.repeat(batch_size).unsqueeze(1)\n",
    "    elif obs_time.ndim == 1:\n",
    "        obs_time = obs_time.repeat(batch_size,1)\n",
    "    obs_time = obs_time.to('cuda')\n",
    "    pow0 = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float32)/d_model).to('cuda')\n",
    "\n",
    "    PE[:, :, 0::2] = torch.sin(torch.einsum('ij,k->ijk', obs_time, pow0))\n",
    "    pow1 = torch.pow(10000, torch.arange(1, d_model, 2, dtype=torch.float32)/d_model).to('cuda')\n",
    "    PE[:, :, 1::2] = torch.cos(torch.einsum('ij,k->ijk', obs_time, pow1))\n",
    "\n",
    "    return PE\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // nhead\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        assert (\n",
    "            d_model % nhead == 0\n",
    "        ), \"Embedding size (d_model) needs to be divisible by number of heads\"\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def attention(self, query, key, value, d_k, mask = None, dropout=None):\n",
    "    \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) /  np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).to('cuda')\n",
    "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "            \n",
    "        output = torch.matmul(scores, value)\n",
    "        return output\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        I = query.shape[0]\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        query = self.q_linear(query).view(I, -1, self.nhead, self.d_k)\n",
    "        key = self.k_linear(key).view(I, -1, self.nhead, self.d_k)\n",
    "        value = self.v_linear(value).view(I, -1, self.nhead, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions I * nhead * J * d_k\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        scores = self.attention(query, key, value, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(I, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c76737d-807c-4e35-a1f7-be79e0e6918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Each expert is a small feed-forward subnetwork.\n",
    "    Here, we map d_model -> d_ff -> d_model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, d_model]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MMoEHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared Expert Pool + 2 Gating Networks (one for each task).\n",
    "    Then each task does a final linear layer to get the actual prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_expert, d_long):\n",
    "        \"\"\"\n",
    "        d_model: dimension of transformer output\n",
    "        d_ff: hidden dimension inside each expert\n",
    "        n_expert: number of experts\n",
    "        d_long: dimension of the longitudinal output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create shared experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff) for _ in range(n_expert)\n",
    "        ])\n",
    "        # Each task has its own gating\n",
    "        self.gate_long = nn.Linear(d_model, n_expert)\n",
    "        self.gate_surv = nn.Linear(d_model, n_expert)\n",
    "\n",
    "        # Final output layers for each task\n",
    "        self.long_out = nn.Linear(d_model, d_long)  # e.g. predict d_long dims\n",
    "        self.surv_out = nn.Linear(d_model, 1)       # e.g. 1-dim survival logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, d_model] from the Transformer.\n",
    "        Returns:\n",
    "          long_pred: [B, T, d_long]\n",
    "          surv_pred: [B, T, 1]\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # 1) Compute each expert's output\n",
    "        #    We'll stack them: shape will be [B, T, n_expert, d_model]\n",
    "        expert_outs = []\n",
    "        for expert in self.experts:\n",
    "            e_out = expert(x)             # [B, T, d_model]\n",
    "            expert_outs.append(e_out)\n",
    "        # Stack along new dim=2 => (n_expert)\n",
    "        # result: [B, T, n_expert, d_model]\n",
    "        expert_outs = torch.stack(expert_outs, dim=2)\n",
    "\n",
    "        # 2) Gating for longitudinal\n",
    "        gate_logits_long = self.gate_long(x)              # [B, T, n_expert]\n",
    "        gate_weights_long = F.softmax(gate_logits_long, dim=-1)  # soft gating\n",
    "        # expand so we can multiply\n",
    "        gate_weights_long = gate_weights_long.unsqueeze(-1)       # [B, T, n_expert, 1]\n",
    "\n",
    "        # Weighted sum over the expert dimension\n",
    "        # shape => [B, T, d_model]\n",
    "        long_combined = (expert_outs * gate_weights_long).sum(dim=2)\n",
    "\n",
    "        # 3) Gating for survival\n",
    "        gate_logits_surv = self.gate_surv(x)              # [B, T, n_expert]\n",
    "        gate_weights_surv = F.softmax(gate_logits_surv, dim=-1)\n",
    "        gate_weights_surv = gate_weights_surv.unsqueeze(-1)       # [B, T, n_expert, 1]\n",
    "\n",
    "        surv_combined = (expert_outs * gate_weights_surv).sum(dim=2)\n",
    "\n",
    "        # 4) Final linear heads for each task\n",
    "        long_pred = self.long_out(long_combined)    # [B, T, d_long]\n",
    "        surv_logit = self.surv_out(surv_combined)    # [B, T, 1]\n",
    "        surv_pred = torch.sigmoid(surv_logit)        # or however you interpret survival\n",
    "\n",
    "        return long_pred, surv_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5046535a-1cc5-44af-aa24-880cdac8cc70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Decoder.forward() missing 1 required positional argument: 'obs_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m pred_times:\n\u001b[1;32m     57\u001b[0m     dec_base \u001b[38;5;241m=\u001b[39m base_0\u001b[38;5;241m.\u001b[39mexpand([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,dec_long\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 59\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(dec_long, dec_base, obs_time)\n\u001b[1;32m     60\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder_pred(out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), out, torch\u001b[38;5;241m.\u001b[39mtensor(pt))\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m#long_out = model.long(out).to('cuda')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#surv_out = torch.sigmoid(model.surv(out)).to('cuda')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: Decoder.forward() missing 1 required positional argument: 'obs_time'"
     ]
    }
   ],
   "source": [
    "from Models.metrics import (AUC, Brier, MSE)\n",
    "minmax_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.fit_transform(train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.transform(test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "train_long, train_base, train_mask, e_train, t_train, train_obs_time = get_tensors(train_data.copy())\n",
    "\n",
    "n_sim = 1\n",
    "i_sim = 0\n",
    "# Initialize arrays for storing results\n",
    "AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "long_mse = np.zeros((n_sim, 3))  # 3 longitudinal outcomes\n",
    "for LT_index, LT in enumerate(landmark_times):\n",
    "    \n",
    "    pred_times = [x+LT for x in pred_windows]\n",
    "    pred_times = pred_times\n",
    "    # Only keep subjects with survival time > landmark time\n",
    "    tmp_data = test_data.loc[test_data[\"time\"]>LT,:]\n",
    "    tmp_id = np.unique(tmp_data[\"id\"].values)\n",
    "    tmp_all = data_all.loc[data_all[\"id\"].isin(tmp_id),:]\n",
    "    \n",
    "    # Only keep longitudinal observations <= landmark time\n",
    "    tmp_data = tmp_data.loc[tmp_data[\"obstime\"]<=LT,:]\n",
    "\n",
    "    true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))\n",
    "    true_prob_LT = tmp_all.loc[tmp_all[\"predtime\"]==LT, [\"true\"]].values\n",
    "    true_prob_tmp = true_prob_tmp / true_prob_LT # true conditional survival\n",
    "            \n",
    "    tmp_long, tmp_base, tmp_mask, e_tmp, t_tmp, obs_time = get_tensors(tmp_data.copy())\n",
    "    tmp_long = tmp_long.to('cuda')\n",
    "    tmp_base = tmp_base.to('cuda')\n",
    "    tmp_mask = tmp_mask.to('cuda')\n",
    "    e_tmp = e_tmp\n",
    "    t_tmp = t_tmp\n",
    "    obs_time = obs_time.to('cuda')\n",
    "    base_0 = tmp_base[:,0,:].unsqueeze(1)        \n",
    "    long_0 = tmp_long\n",
    "    mask_T = torch.ones((long_0.shape[0],1), dtype=torch.bool).to('cuda')\n",
    "    \n",
    "    dec_long = long_0\n",
    "    dec_base = base_0\n",
    "    \n",
    "    long_pred = torch.zeros(long_0.shape[0],0,long_0.shape[2]).to('cuda')\n",
    "    surv_pred = torch.zeros(long_0.shape[0],0,1).to('cuda')\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    for pt in pred_times:\n",
    "        dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "        \n",
    "        out = model.decoder(dec_long, dec_base, obs_time)\n",
    "        out = model.decoder_pred(out[:,-1,:].unsqueeze(1), out, torch.tensor(pt))\n",
    "        #long_out = model.long(out).to('cuda')\n",
    "        #surv_out = torch.sigmoid(model.surv(out)).to('cuda')\n",
    "        long_out, surv_out = model.mmoe_head(out)\n",
    "        long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "        surv_pred = torch.cat((surv_pred, surv_out), dim=1)\n",
    "        \n",
    "        dec_long = torch.cat((dec_long, long_out), dim=1)\n",
    "        tmp_mask = torch.cat((tmp_mask, mask_T), dim=1)\n",
    "        obs_time = torch.cat((obs_time, torch.tensor(pt).expand([obs_time.shape[0],1]).to('cuda')),dim=1)\n",
    "    \n",
    "    long_pred = long_pred.detach().cpu().numpy()\n",
    "    surv_pred = surv_pred.squeeze().detach().cpu().numpy()\n",
    "    surv_pred = surv_pred.cumprod(axis=1)\n",
    "\n",
    "    auc, iauc = AUC(surv_pred, e_tmp.numpy(), t_tmp.numpy(), np.array(pred_times))\n",
    "    AUC_array[i_sim, LT_index, :] = auc\n",
    "    iAUC_array[i_sim, LT_index] = iauc\n",
    "    auc, iauc = AUC(true_prob_tmp, np.array(e_tmp), np.array(t_tmp), np.array(pred_times))\n",
    "    true_AUC_array[i_sim, LT_index, :] = auc\n",
    "    true_iAUC_array[i_sim, LT_index] = iauc\n",
    "    \n",
    "    bs, ibs = Brier(surv_pred, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                      e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "    BS_array[i_sim, LT_index, :] = bs\n",
    "    iBS_array[i_sim, LT_index] = ibs\n",
    "    bs, ibs = Brier(true_prob_tmp, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                      e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "    true_BS_array[i_sim, LT_index, :] = bs\n",
    "    true_iBS_array[i_sim, LT_index] = ibs\n",
    "    \n",
    "    \n",
    "\n",
    "## Longitudinal Prediction for observed values\n",
    "test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for j in range(1,test_long.shape[1]):\n",
    "    dec_long = test_long[:,:j,:]\n",
    "    dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "\n",
    "    out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "    out = model.decoder_pred(out[:,-1,:].unsqueeze(1).to('cuda'), out, obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    long_out = model.long(out)\n",
    "    \n",
    "    long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "long_pred = long_pred.detach().cpu().numpy()\n",
    "long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "\n",
    "long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "long_obs = long_obs.filled(fill_value=np.nan)\n",
    "\n",
    "long_mse[i_sim,:] = MSE(long_pred, long_obs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"AUC:\",np.nanmean(AUC_array, axis=0))\n",
    "print(\"iAUC:\",np.mean(iAUC_array, axis=0))\n",
    "print(\"True AUC:\",np.nanmean(true_AUC_array, axis=0))\n",
    "print(\"True iAUC:\",np.mean(true_iAUC_array, axis=0))\n",
    "print(\"Difference:\",np.mean(true_iAUC_array, axis=0) - np.mean(iAUC_array, axis=0))\n",
    "\n",
    "print(\"BS:\\n\", np.mean(BS_array, axis=0))\n",
    "print(\"iBS:\",np.mean(iBS_array, axis=0))\n",
    "print(\"True BS:\\n\", np.mean(true_BS_array, axis=0))\n",
    "print(\"True iBS:\",np.mean(true_iBS_array, axis=0))\n",
    "\n",
    "print(\"Long MSE:\",np.mean(long_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadc86c-7dd9-4c7a-a873-2219fcf07ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat293",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
