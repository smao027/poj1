{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from Gate import Transformer2\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "from util import (long_loss, surv_loss)\n",
    "model = Transformer2(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2,mask,s,e= surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shijimao/Proj1/util.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"id_new\"] = df.groupby(by=\"id\").ngroup()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a9af43407f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Gate import Transformer2\n",
    "import torch\n",
    "import numpy as np\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "import matplotlib.pyplot as plt\n",
    "from util import (long_loss, surv_loss)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load the pickle file back into a DataFrame\n",
    "data = pd.read_pickle('data/simulated_data.pkl')\n",
    "I = data['id'].nunique()\n",
    "\n",
    "        ## split train/test\n",
    "random_id = range(I) #np.random.permutation(range(I))\n",
    "train_id = random_id[0:int(0.7*I)]\n",
    "test_id = random_id[int(0.7*I):I]\n",
    "\n",
    "train_data = data[data[\"id\"].isin(train_id)]\n",
    "test_data = data[data[\"id\"].isin(test_id)]\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(train_data)\n",
    "minmax_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.fit_transform(train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.transform(test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gate import Transformer1\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "model = Transformer1(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from Gate import Transformer3\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "model = Transformer3(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import (AUC, Brier, MSE)\n",
    "n_sim = 0\n",
    "long_mse = np.zeros((1, 3))  \n",
    "\n",
    "test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for j in range(1,test_long.shape[1]):\n",
    "    dec_long = test_long[:,:j,:]\n",
    "    print(dec_long.shape)\n",
    "    dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    print(dec_base.shape)\n",
    "    out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "    #long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    out = model.decoder_pred(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    long_out = model.long(out)\n",
    "    long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "long_pred = long_pred.detach().cpu().numpy()\n",
    "long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "\n",
    "long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "long_obs = long_obs.filled(fill_value=np.nan)\n",
    "\n",
    "long_mse[0,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import (AUC, Brier, MSE)\n",
    "n_sim = 0\n",
    "long_mse = np.zeros((1, 3))  \n",
    "\n",
    "test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for j in range(1,test_long.shape[1]):\n",
    "    dec_long = test_long[:,:j,:]\n",
    "    dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "    long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "long_pred = long_pred.detach().cpu().numpy()\n",
    "long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "\n",
    "long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "long_obs = long_obs.filled(fill_value=np.nan)\n",
    "\n",
    "long_mse[0,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 794 into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Only keep longitudinal observations <= landmark time\u001b[39;00m\n\u001b[1;32m     83\u001b[0m tmp_data \u001b[38;5;241m=\u001b[39m tmp_data\u001b[38;5;241m.\u001b[39mloc[tmp_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobstime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mLT,:]\n\u001b[0;32m---> 85\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n\u001b[1;32m     86\u001b[0m true_prob_LT \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mLT, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     87\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m true_prob_tmp \u001b[38;5;241m/\u001b[39m true_prob_LT \u001b[38;5;66;03m# true conditional survival\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 794 into shape (3)"
     ]
    }
   ],
   "source": [
    "\n",
    "from Gate import Transformer2\n",
    "\n",
    "model = Transformer2(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                num_decoder_layers=7)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "train_long, train_base, train_mask, e_train, t_train, train_obs_time = get_tensors(train_data.copy())\n",
    "n_epoch = n_epoch \n",
    "batch_size = batch_size \n",
    "n_sim = 2\n",
    "obstime = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "landmark_times = [1,2,3,4,5]\n",
    "pred_windows = [1,2,3] \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "long_mse = np.zeros((n_sim, 3)) \n",
    "\n",
    "for i_sim in range(n_sim):\n",
    "    if i_sim % 10 == 0:\n",
    "        print(i_sim)\n",
    "\n",
    "    np.random.seed(i_sim)  \n",
    "    ### \n",
    "    ### Train \n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss = 0\n",
    "        train_id = np.random.permutation(train_id)\n",
    "        for batch in range(0, len(train_id), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            indices = train_id[batch:batch+batch_size]\n",
    "            batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "                \n",
    "            batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "            batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "            batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "            batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "            batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "            obs_time = obs_time.to('cuda')\n",
    "            yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                            obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "            \n",
    "            loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "            loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "            \n",
    "            loss = loss1 + loss2\n",
    "            \n",
    "            loss.backward()\n",
    "            scheduler.step()\n",
    "            running_loss += loss\n",
    "        loss_values.append(running_loss.tolist())\n",
    "\n",
    "\n",
    "    ### \n",
    "    ### Test \n",
    "\n",
    "    ## Survival Prediction using Landmarking\n",
    "\n",
    "    for LT_index, LT in enumerate(landmark_times):\n",
    "        \n",
    "        pred_times = [x+LT for x in pred_windows]\n",
    "        pred_times = pred_times\n",
    "        # Only keep subjects with survival time > landmark time\n",
    "        tmp_data = test_data.loc[test_data[\"time\"]>LT,:]\n",
    "        tmp_id = np.unique(tmp_data[\"id\"].values)\n",
    "        tmp_all = data.loc[data[\"id\"].isin(tmp_id),:]\n",
    "        \n",
    "        # Only keep longitudinal observations <= landmark time\n",
    "        tmp_data = tmp_data.loc[tmp_data[\"obstime\"]<=LT,:]\n",
    "\n",
    "        true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))\n",
    "        true_prob_LT = tmp_all.loc[tmp_all[\"predtime\"]==LT, [\"true\"]].values\n",
    "        true_prob_tmp = true_prob_tmp / true_prob_LT # true conditional survival\n",
    "                \n",
    "        tmp_long, tmp_base, tmp_mask, e_tmp, t_tmp, obs_time = get_tensors(tmp_data.copy())\n",
    "        tmp_long = tmp_long.to('cuda')\n",
    "        tmp_base = tmp_base.to('cuda')\n",
    "        tmp_mask = tmp_mask.to('cuda')\n",
    "        e_tmp = e_tmp\n",
    "        t_tmp = t_tmp\n",
    "        obs_time = obs_time.to('cuda')\n",
    "        base_0 = tmp_base[:,0,:].unsqueeze(1)        \n",
    "        long_0 = tmp_long\n",
    "        mask_T = torch.ones((long_0.shape[0],1), dtype=torch.bool).to('cuda')\n",
    "        \n",
    "        dec_long = long_0\n",
    "        dec_base = base_0\n",
    "        \n",
    "        long_pred = torch.zeros(long_0.shape[0],0,long_0.shape[2]).to('cuda')\n",
    "        surv_pred = torch.zeros(long_0.shape[0],0,1).to('cuda')\n",
    "        \n",
    "        model = model.eval()\n",
    "        \n",
    "        for pt in pred_times:\n",
    "            dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "            \n",
    "            out = model.decoder(dec_long, dec_base, get_mask(tmp_mask), obs_time)\n",
    "            \n",
    "            long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "            long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "            \n",
    "            dec_long = torch.cat((dec_long, long_out), dim=1)\n",
    "            tmp_mask = torch.cat((tmp_mask, mask_T), dim=1)\n",
    "            obs_time = torch.cat((obs_time, torch.tensor(pt).expand([obs_time.shape[0],1]).to('cuda')),dim=1)\n",
    "        \n",
    "        long_pred = long_pred.detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.squeeze().detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.cumprod(axis=1)\n",
    "\n",
    "        auc, iauc = AUC(surv_pred, e_tmp.numpy(), t_tmp.numpy(), np.array(pred_times))\n",
    "        AUC_array[i_sim, LT_index, :] = auc\n",
    "        iAUC_array[i_sim, LT_index] = iauc\n",
    "        auc, iauc = AUC(true_prob_tmp, np.array(e_tmp), np.array(t_tmp), np.array(pred_times))\n",
    "        true_AUC_array[i_sim, LT_index, :] = auc\n",
    "        true_iAUC_array[i_sim, LT_index] = iauc\n",
    "        \n",
    "        bs, ibs = Brier(surv_pred, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        BS_array[i_sim, LT_index, :] = bs\n",
    "        iBS_array[i_sim, LT_index] = ibs\n",
    "        bs, ibs = Brier(true_prob_tmp, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        true_BS_array[i_sim, LT_index, :] = bs\n",
    "        true_iBS_array[i_sim, LT_index] = ibs\n",
    "        \n",
    "    \n",
    "    ## Longitudinal Prediction for observed values\n",
    "    test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "    base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "    long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    for j in range(1,test_long.shape[1]):\n",
    "        dec_long = test_long[:,:j,:]\n",
    "        dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    \n",
    "        out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "        long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "        long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "    \n",
    "    long_pred = long_pred.detach().cpu().numpy()\n",
    "    long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "    long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "    \n",
    "    long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "    long_obs = long_obs.fillMyModeled(fill_value=np.nan)\n",
    "    \n",
    "    long_mse[i_sim,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 794 into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Only keep longitudinal observations <= landmark time\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tmp_data \u001b[38;5;241m=\u001b[39m tmp_data\u001b[38;5;241m.\u001b[39mloc[tmp_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobstime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mLT,:]\n\u001b[0;32m---> 13\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n\u001b[1;32m     14\u001b[0m true_prob_LT \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mLT, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     15\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m true_prob_tmp \u001b[38;5;241m/\u001b[39m true_prob_LT \u001b[38;5;66;03m# true conditional survival\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 794 into shape (3)"
     ]
    }
   ],
   "source": [
    "for LT_index, LT in enumerate(landmark_times):\n",
    "        \n",
    "        pred_times = [x+LT for x in pred_windows]\n",
    "        pred_times = pred_times\n",
    "        # Only keep subjects with survival time > landmark time\n",
    "        tmp_data = test_data.loc[test_data[\"time\"]>LT,:]\n",
    "        tmp_id = np.unique(tmp_data[\"id\"].values)\n",
    "        tmp_all = data.loc[data[\"id\"].isin(tmp_id),:]\n",
    "        \n",
    "        # Only keep longitudinal observations <= landmark time\n",
    "        tmp_data = tmp_data.loc[tmp_data[\"obstime\"]<=LT,:]\n",
    "        \n",
    "        true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))\n",
    "        true_prob_LT = tmp_all.loc[tmp_all[\"predtime\"]==LT, [\"true\"]].values\n",
    "        true_prob_tmp = true_prob_tmp / true_prob_LT # true conditional survival\n",
    "                \n",
    "        tmp_long, tmp_base, tmp_mask, e_tmp, t_tmp, obs_time = get_tensors(tmp_data.copy())\n",
    "        tmp_long = tmp_long.to('cuda')\n",
    "        tmp_base = tmp_base.to('cuda')\n",
    "        tmp_mask = tmp_mask.to('cuda')\n",
    "        e_tmp = e_tmp\n",
    "        t_tmp = t_tmp\n",
    "        obs_time = obs_time.to('cuda')\n",
    "        base_0 = tmp_base[:,0,:].unsqueeze(1)        \n",
    "        long_0 = tmp_long\n",
    "        mask_T = torch.ones((long_0.shape[0],1), dtype=torch.bool).to('cuda')\n",
    "        \n",
    "        dec_long = long_0\n",
    "        dec_base = base_0\n",
    "        \n",
    "        long_pred = torch.zeros(long_0.shape[0],0,long_0.shape[2]).to('cuda')\n",
    "        surv_pred = torch.zeros(long_0.shape[0],0,1).to('cuda')\n",
    "        \n",
    "        model = model.eval()\n",
    "        \n",
    "        for pt in pred_times:\n",
    "            dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "            \n",
    "            out = model.decoder(dec_long, dec_base, get_mask(tmp_mask), obs_time)\n",
    "            long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "            long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "            \n",
    "            dec_long = torch.cat((dec_long, long_out), dim=1)\n",
    "            tmp_mask = torch.cat((tmp_mask, mask_T), dim=1)\n",
    "            obs_time = torch.cat((obs_time, torch.tensor(pt).expand([obs_time.shape[0],1]).to('cuda')),dim=1)\n",
    "        \n",
    "        long_pred = long_pred.detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.squeeze().detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.cumprod(axis=1)\n",
    "\n",
    "        auc, iauc = AUC(surv_pred, e_tmp.numpy(), t_tmp.numpy(), np.array(pred_times))\n",
    "        AUC_array[i_sim, LT_index, :] = auc\n",
    "        iAUC_array[i_sim, LT_index] = iauc\n",
    "        auc, iauc = AUC(true_prob_tmp, np.array(e_tmp), np.array(t_tmp), np.array(pred_times))\n",
    "        true_AUC_array[i_sim, LT_index, :] = auc\n",
    "        true_iAUC_array[i_sim, LT_index] = iauc\n",
    "        \n",
    "        bs, ibs = Brier(surv_pred, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        BS_array[i_sim, LT_index, :] = bs\n",
    "        iBS_array[i_sim, LT_index] = ibs\n",
    "        bs, ibs = Brier(true_prob_tmp, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        true_BS_array[i_sim, LT_index, :] = bs\n",
    "        true_iBS_array[i_sim, LT_index] = ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 794 into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 794 into shape (3)"
     ]
    }
   ],
   "source": [
    "true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(794, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 794 into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 794 into shape (3)"
     ]
    }
   ],
   "source": [
    "tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat293",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
