{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from Gate import Transformer2\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "from util import (long_loss, surv_loss)\n",
    "model = Transformer2(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8725, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shijimao/Proj1/util.py:191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"id_new\"] = df.groupby(by=\"id\").ngroup()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x725ef6b587f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Gate import Transformer2\n",
    "import torch\n",
    "import numpy as np\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "import matplotlib.pyplot as plt\n",
    "from util import (long_loss, surv_loss)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Load the pickle file back into a DataFrame\n",
    "data = pd.read_pickle('data/DIVAT_sim_1000_visit_1000_long_3_seed_123.pkl')\n",
    "I = data['id'].nunique()\n",
    "\n",
    "        ## split train/test\n",
    "random_id = range(I) #np.random.permutation(range(I))\n",
    "train_id = random_id[0:int(0.7*I)]\n",
    "test_id = random_id[int(0.7*I):I]\n",
    "\n",
    "train_data = data[data[\"id\"].isin(train_id)]\n",
    "test_data = data[data[\"id\"].isin(test_id)]\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(train_data)\n",
    "minmax_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.fit_transform(train_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]] = minmax_scaler.transform(test_data.loc[:,[\"Y1\",\"Y2\",\"Y3\"]])\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gate import Transformer1\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "model = Transformer1(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from Gate import Transformer3\n",
    "from util import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "model = Transformer3(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=4)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "\n",
    "n_epoch = 25\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "    loss_values.append(running_loss.tolist())\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 1, 3])\n",
      "torch.Size([300, 1, 2])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transformer2' object has no attribute 'decoder_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(dec_long\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), dec_base\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), get_mask(test_mask[:,:j])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), obs_time[:,:j]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder_pred(out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), out, test_mask[:,:j]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), obs_time[:,j]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m long_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlong(out)\n\u001b[1;32m     20\u001b[0m long_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((long_pred, long_out), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer2' object has no attribute 'decoder_pred'"
     ]
    }
   ],
   "source": [
    "from metrics import (AUC, Brier, MSE)\n",
    "n_sim = 0\n",
    "long_mse = np.zeros((1, 3))  \n",
    "\n",
    "test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for j in range(1,test_long.shape[1]):\n",
    "    dec_long = test_long[:,:j,:]\n",
    "    print(dec_long.shape)\n",
    "    dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    print(dec_base.shape)\n",
    "    out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "    #long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    out = model.decoder_pred(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    long_out = model.long(out)\n",
    "    long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "long_pred = long_pred.detach().cpu().numpy()\n",
    "long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "\n",
    "long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "long_obs = long_obs.filled(fill_value=np.nan)\n",
    "\n",
    "long_mse[0,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import (AUC, Brier, MSE)\n",
    "n_sim = 0\n",
    "long_mse = np.zeros((1, 3))  \n",
    "\n",
    "test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for j in range(1,test_long.shape[1]):\n",
    "    dec_long = test_long[:,:j,:]\n",
    "    dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "    long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "    long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "long_pred = long_pred.detach().cpu().numpy()\n",
    "long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "\n",
    "long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "long_obs = long_obs.filled(fill_value=np.nan)\n",
    "\n",
    "long_mse[0,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'predtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predtime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Only keep longitudinal observations <= landmark time\u001b[39;00m\n\u001b[1;32m     83\u001b[0m tmp_data \u001b[38;5;241m=\u001b[39m tmp_data\u001b[38;5;241m.\u001b[39mloc[tmp_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobstime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mLT,:]\n\u001b[0;32m---> 85\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n\u001b[1;32m     86\u001b[0m true_prob_LT \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mLT, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     87\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m true_prob_tmp \u001b[38;5;241m/\u001b[39m true_prob_LT \u001b[38;5;66;03m# true conditional survival\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predtime'"
     ]
    }
   ],
   "source": [
    "\n",
    "from Gate import Transformer2\n",
    "\n",
    "model = Transformer2(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                num_decoder_layers=7)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "train_long, train_base, train_mask, e_train, t_train, train_obs_time = get_tensors(train_data.copy())\n",
    "n_epoch = n_epoch \n",
    "batch_size = batch_size \n",
    "n_sim = 2\n",
    "obstime = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "landmark_times = [1,2,3,4,5]\n",
    "pred_windows = [1,2,3] \n",
    "    \n",
    "loss_values = []\n",
    "\n",
    "AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_AUC_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iAUC_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "true_BS_array = np.zeros((n_sim, len(landmark_times), len(pred_windows)))\n",
    "true_iBS_array = np.zeros((n_sim, len(landmark_times)))\n",
    "\n",
    "long_mse = np.zeros((n_sim, 3)) \n",
    "\n",
    "for i_sim in range(n_sim):\n",
    "    if i_sim % 10 == 0:\n",
    "        print(i_sim)\n",
    "\n",
    "    np.random.seed(i_sim)  \n",
    "    ### \n",
    "    ### Train \n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss = 0\n",
    "        train_id = np.random.permutation(train_id)\n",
    "        for batch in range(0, len(train_id), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            indices = train_id[batch:batch+batch_size]\n",
    "            batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "                \n",
    "            batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "            batch_long_inp = batch_long[:,:-1,:].to('cuda');batch_long_out = batch_long[:,1:,:].to('cuda')  #time 1-11 as train and 12 as validation \n",
    "            batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "            batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "            batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "            obs_time = obs_time.to('cuda')\n",
    "            yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                            obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "            \n",
    "            loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "            loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "            \n",
    "            loss = loss1 + loss2\n",
    "            \n",
    "            loss.backward()\n",
    "            scheduler.step()\n",
    "            running_loss += loss\n",
    "        loss_values.append(running_loss.tolist())\n",
    "\n",
    "\n",
    "    ### \n",
    "    ### Test \n",
    "\n",
    "    ## Survival Prediction using Landmarking\n",
    "\n",
    "    for LT_index, LT in enumerate(landmark_times):\n",
    "        \n",
    "        pred_times = [x+LT for x in pred_windows]\n",
    "        pred_times = pred_times\n",
    "        # Only keep subjects with survival time > landmark time\n",
    "        tmp_data = test_data.loc[test_data[\"time\"]>LT,:]\n",
    "        tmp_id = np.unique(tmp_data[\"id\"].values)\n",
    "        tmp_all = data.loc[data[\"id\"].isin(tmp_id),:]\n",
    "        \n",
    "        # Only keep longitudinal observations <= landmark time\n",
    "        tmp_data = tmp_data.loc[tmp_data[\"obstime\"]<=LT,:]\n",
    "\n",
    "        true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))\n",
    "        true_prob_LT = tmp_all.loc[tmp_all[\"predtime\"]==LT, [\"true\"]].values\n",
    "        true_prob_tmp = true_prob_tmp / true_prob_LT # true conditional survival\n",
    "                \n",
    "        tmp_long, tmp_base, tmp_mask, e_tmp, t_tmp, obs_time = get_tensors(tmp_data.copy())\n",
    "        tmp_long = tmp_long.to('cuda')\n",
    "        tmp_base = tmp_base.to('cuda')\n",
    "        tmp_mask = tmp_mask.to('cuda')\n",
    "        e_tmp = e_tmp\n",
    "        t_tmp = t_tmp\n",
    "        obs_time = obs_time.to('cuda')\n",
    "        base_0 = tmp_base[:,0,:].unsqueeze(1)        \n",
    "        long_0 = tmp_long\n",
    "        mask_T = torch.ones((long_0.shape[0],1), dtype=torch.bool).to('cuda')\n",
    "        \n",
    "        dec_long = long_0\n",
    "        dec_base = base_0\n",
    "        \n",
    "        long_pred = torch.zeros(long_0.shape[0],0,long_0.shape[2]).to('cuda')\n",
    "        surv_pred = torch.zeros(long_0.shape[0],0,1).to('cuda')\n",
    "        \n",
    "        model = model.eval()\n",
    "        \n",
    "        for pt in pred_times:\n",
    "            dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "            \n",
    "            out = model.decoder(dec_long, dec_base, get_mask(tmp_mask), obs_time)\n",
    "            \n",
    "            long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "            long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "            \n",
    "            dec_long = torch.cat((dec_long, long_out), dim=1)\n",
    "            tmp_mask = torch.cat((tmp_mask, mask_T), dim=1)\n",
    "            obs_time = torch.cat((obs_time, torch.tensor(pt).expand([obs_time.shape[0],1]).to('cuda')),dim=1)\n",
    "        \n",
    "        long_pred = long_pred.detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.squeeze().detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.cumprod(axis=1)\n",
    "\n",
    "        auc, iauc = AUC(surv_pred, e_tmp.numpy(), t_tmp.numpy(), np.array(pred_times))\n",
    "        AUC_array[i_sim, LT_index, :] = auc\n",
    "        iAUC_array[i_sim, LT_index] = iauc\n",
    "        auc, iauc = AUC(true_prob_tmp, np.array(e_tmp), np.array(t_tmp), np.array(pred_times))\n",
    "        true_AUC_array[i_sim, LT_index, :] = auc\n",
    "        true_iAUC_array[i_sim, LT_index] = iauc\n",
    "        \n",
    "        bs, ibs = Brier(surv_pred, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        BS_array[i_sim, LT_index, :] = bs\n",
    "        iBS_array[i_sim, LT_index] = ibs\n",
    "        bs, ibs = Brier(true_prob_tmp, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        true_BS_array[i_sim, LT_index, :] = bs\n",
    "        true_iBS_array[i_sim, LT_index] = ibs\n",
    "        \n",
    "    \n",
    "    ## Longitudinal Prediction for observed values\n",
    "    test_long, test_base, test_mask, e_test, t_test, obs_time = get_tensors(test_data.copy())    \n",
    "    base_0 = test_base[:,0,:].unsqueeze(1)\n",
    "    long_pred = torch.zeros(test_long.shape[0],0,test_long.shape[2]).to('cuda')\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    for j in range(1,test_long.shape[1]):\n",
    "        dec_long = test_long[:,:j,:]\n",
    "        dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "    \n",
    "        out = model.decoder(dec_long.to('cuda'), dec_base.to('cuda'), get_mask(test_mask[:,:j]).to('cuda'), obs_time[:,:j].to('cuda'))\n",
    "        long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "        long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "\n",
    "    \n",
    "    long_pred = long_pred.detach().cpu().numpy()\n",
    "    long_obs = test_long[:,1:,:].cpu().numpy()\n",
    "    long_mask = test_mask[:,1:].unsqueeze(2).repeat((1,1,long_pred.shape[-1])).cpu().numpy()\n",
    "    \n",
    "    long_obs = np.ma.array(long_obs, mask=1-long_mask)\n",
    "    long_obs = long_obs.fillMyModeled(fill_value=np.nan)\n",
    "    \n",
    "    long_mse[i_sim,:] = MSE(long_pred, long_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 794 into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Only keep longitudinal observations <= landmark time\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tmp_data \u001b[38;5;241m=\u001b[39m tmp_data\u001b[38;5;241m.\u001b[39mloc[tmp_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobstime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mLT,:]\n\u001b[0;32m---> 13\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pred_times), [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(pred_times))\n\u001b[1;32m     14\u001b[0m true_prob_LT \u001b[38;5;241m=\u001b[39m tmp_all\u001b[38;5;241m.\u001b[39mloc[tmp_all[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredtime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mLT, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     15\u001b[0m true_prob_tmp \u001b[38;5;241m=\u001b[39m true_prob_tmp \u001b[38;5;241m/\u001b[39m true_prob_LT \u001b[38;5;66;03m# true conditional survival\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 794 into shape (3)"
     ]
    }
   ],
   "source": [
    "for LT_index, LT in enumerate(landmark_times):\n",
    "        \n",
    "        pred_times = [x+LT for x in pred_windows]\n",
    "        pred_times = pred_times\n",
    "        # Only keep subjects with survival time > landmark time\n",
    "        tmp_data = test_data.loc[test_data[\"time\"]>LT,:]\n",
    "        tmp_id = np.unique(tmp_data[\"id\"].values)\n",
    "        tmp_all = data.loc[data[\"id\"].isin(tmp_id),:]\n",
    "        \n",
    "        # Only keep longitudinal observations <= landmark time\n",
    "        tmp_data = tmp_data.loc[tmp_data[\"obstime\"]<=LT,:]\n",
    "        \n",
    "        true_prob_tmp = tmp_all.loc[tmp_all[\"predtime\"].isin(pred_times), [\"true\"]].values.reshape(-1,len(pred_times))\n",
    "        true_prob_LT = tmp_all.loc[tmp_all[\"predtime\"]==LT, [\"true\"]].values\n",
    "        true_prob_tmp = true_prob_tmp / true_prob_LT # true conditional survival\n",
    "                \n",
    "        tmp_long, tmp_base, tmp_mask, e_tmp, t_tmp, obs_time = get_tensors(tmp_data.copy())\n",
    "        tmp_long = tmp_long.to('cuda')\n",
    "        tmp_base = tmp_base.to('cuda')\n",
    "        tmp_mask = tmp_mask.to('cuda')\n",
    "        e_tmp = e_tmp\n",
    "        t_tmp = t_tmp\n",
    "        obs_time = obs_time.to('cuda')\n",
    "        base_0 = tmp_base[:,0,:].unsqueeze(1)        \n",
    "        long_0 = tmp_long\n",
    "        mask_T = torch.ones((long_0.shape[0],1), dtype=torch.bool).to('cuda')\n",
    "        \n",
    "        dec_long = long_0\n",
    "        dec_base = base_0\n",
    "        \n",
    "        long_pred = torch.zeros(long_0.shape[0],0,long_0.shape[2]).to('cuda')\n",
    "        surv_pred = torch.zeros(long_0.shape[0],0,1).to('cuda')\n",
    "        \n",
    "        model = model.eval()\n",
    "        \n",
    "        for pt in pred_times:\n",
    "            dec_base = base_0.expand([-1,dec_long.shape[1],-1])\n",
    "            \n",
    "            out = model.decoder(dec_long, dec_base, get_mask(tmp_mask), obs_time)\n",
    "            long_out,surv_out = model.mmoe_layer(out[:,-1,:].unsqueeze(1).to('cuda'), out, test_mask[:,:j].unsqueeze(1).to('cuda'), obs_time[:,j].unsqueeze(1).to('cuda'))\n",
    "            long_pred = torch.cat((long_pred, long_out), dim=1)\n",
    "            \n",
    "            dec_long = torch.cat((dec_long, long_out), dim=1)\n",
    "            tmp_mask = torch.cat((tmp_mask, mask_T), dim=1)\n",
    "            obs_time = torch.cat((obs_time, torch.tensor(pt).expand([obs_time.shape[0],1]).to('cuda')),dim=1)\n",
    "        \n",
    "        long_pred = long_pred.detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.squeeze().detach().cpu().numpy()\n",
    "        surv_pred = surv_pred.cumprod(axis=1)\n",
    "\n",
    "        auc, iauc = AUC(surv_pred, e_tmp.numpy(), t_tmp.numpy(), np.array(pred_times))\n",
    "        AUC_array[i_sim, LT_index, :] = auc\n",
    "        iAUC_array[i_sim, LT_index] = iauc\n",
    "        auc, iauc = AUC(true_prob_tmp, np.array(e_tmp), np.array(t_tmp), np.array(pred_times))\n",
    "        true_AUC_array[i_sim, LT_index, :] = auc\n",
    "        true_iAUC_array[i_sim, LT_index] = iauc\n",
    "        \n",
    "        bs, ibs = Brier(surv_pred, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        BS_array[i_sim, LT_index, :] = bs\n",
    "        iBS_array[i_sim, LT_index] = ibs\n",
    "        bs, ibs = Brier(true_prob_tmp, e_tmp.numpy(), t_tmp.numpy(),\n",
    "                            e_train.numpy(), t_train.numpy(), LT, np.array(pred_windows))\n",
    "        true_BS_array[i_sim, LT_index, :] = bs\n",
    "        true_iBS_array[i_sim, LT_index] = ibs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat293",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
