{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MMoE_FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts Feed-Forward Network (MMoE_FFN).\n",
    "    Contains:\n",
    "      - Router (gating) linear layer to produce softmax weights for N experts.\n",
    "      - N expert feed-forward networks (each an MLP).\n",
    "      - Two task-specific linear heads (tower networks) for predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, hidden_dim: int, num_experts: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input features (and of expert outputs).\n",
    "            hidden_dim: Hidden layer dimension for each expert MLP.\n",
    "            num_experts: Number of expert networks.\n",
    "            dropout: Dropout probability for expert hidden layers.\n",
    "        \"\"\"\n",
    "        super(MMoE_FFN, self).__init__()\n",
    "        # Gating network: linear layer that outputs N logits per token (one per expert)\n",
    "        self.router = nn.Linear(d_model, num_experts)\n",
    "        # Expert networks: each is a two-layer feed-forward (d_model -> hidden_dim -> d_model)\n",
    "        # We use ReLU activation and dropout on the hidden layer for each expert.\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, d_model)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Task-specific tower heads: linear layers for each task\n",
    "        self.longitudinal_head = nn.Linear(d_model, 3)  # 3 outputs for longitudinal task\n",
    "        self.survival_head    = nn.Linear(d_model, 1)  # 1 output for survival task\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor [B, S, d_model] – input features (e.g., from attention output).\n",
    "        Returns:\n",
    "            combined_out: Tensor [B, S, d_model] – combined expert output for each token.\n",
    "            long_out: Tensor [B, S, 3] – longitudinal task predictions for each token.\n",
    "            surv_out: Tensor [B, S, 1] – survival task predictions for each token.\n",
    "        \"\"\"\n",
    "        B, S, _ = x.shape\n",
    "        # Gating: compute softmax weights for each expert per token\n",
    "        # router(x) -> [B, S, N], then softmax along the N dimension to get probabilities\n",
    "        gating_logits = self.router(x)                                # [B, S, N]\n",
    "        gating_weights = F.softmax(gating_logits, dim=-1)             # [B, S, N] (sum of N dim = 1 per token)&#8203;:contentReference[oaicite:4]{index=4}\n",
    "        \n",
    "        # Compute each expert's output on x\n",
    "        # (Each expert is applied position-wise on the sequence)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]       # list of [B, S, d_model]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)           # [B, S, N, d_model]\n",
    "        \n",
    "        # Mixture: weight and sum expert outputs using gating weights\n",
    "        # combined_out[b,s,:] = Σ_{i=0}^{N-1} gating_weights[b,s,i] * expert_outputs[b,s,i,:]&#8203;:contentReference[oaicite:5]{index=5}\n",
    "        combined_out = (gating_weights.unsqueeze(-1) * expert_outputs).sum(dim=2)  # [B, S, d_model]\n",
    "        \n",
    "        # Task-specific outputs from the combined representation (tower networks)\n",
    "        long_out = self.longitudinal_head(combined_out)  # [B, S, 3]  (logits or scores for longitudinal task)&#8203;:contentReference[oaicite:6]{index=6}\n",
    "        surv_out = self.survival_head(combined_out)      # [B, S, 1]  (logit or score for survival task)&#8203;:contentReference[oaicite:7]{index=7}\n",
    "        return long_out, surv_out\n",
    "\n",
    "class Decoder_Layer2(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder Layer with MMoE.\n",
    "    - Self-attention sub-layer (with residual connection + norm)\n",
    "    - MMoE feed-forward sub-layer (with residual connection + norm)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, hidden_dim: int, num_experts: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (embedding size for inputs).\n",
    "            nhead: Number of heads for multi-head self-attention.\n",
    "            hidden_dim: Hidden dimension for each expert FFN.\n",
    "            num_experts: Number of expert networks in the MMoE FFN.\n",
    "            dropout: Dropout probability for attention and FFN.\n",
    "        \"\"\"\n",
    "        super(Decoder_Layer2, self).__init__()\n",
    "        # Multi-head self-attention (batch_first=True for [B, S, d_model] inputs/outputs)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        # Mixture-of-Experts feed-forward network\n",
    "        self.mmoe_ffn = MMoE_FFN(d_model, hidden_dim, num_experts, dropout=dropout)\n",
    "        # LayerNorm for post-attention and post-FFN\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers for residual connections\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor [B, S, d_model] – input sequence to decode (encoder output).\n",
    "            attn_mask: Optional mask for attention. \n",
    "                       - If shape [B, S], it is used as a key padding mask (True for positions to mask out).\n",
    "                       - If shape [S, S] (or [B, S, S]), it is used as an attention mask (e.g., causal mask).\n",
    "        Returns:\n",
    "            out: Tensor [B, S, d_model] – output representation after this layer.\n",
    "            long_out: Tensor [B, S, 3] – longitudinal task output for each token.\n",
    "            surv_out: Tensor [B, S, 1] – survival task output for each token.\n",
    "        \"\"\"\n",
    "        # 1. Self-Attention sub-layer\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2 and attn_mask.shape == (x.size(0), x.size(1)):\n",
    "                # Treat 2D mask of shape [B, S] as key padding mask\n",
    "                attn_out, _ = self.self_attn(x, x, x, key_padding_mask=attn_mask, need_weights=False)\n",
    "            else:\n",
    "                # Treat as an attn_mask (shape [S, S] or [B, S, S])\n",
    "                attn_out, _ = self.self_attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        else:\n",
    "            attn_out, _ = self.self_attn(x, x, x, need_weights=False)\n",
    "        # Add & Norm: residual connection and LayerNorm for attention output&#8203;:contentReference[oaicite:8]{index=8}\n",
    "        x = x + self.dropout_attn(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # 2. Mixture-of-Experts FFN sub-layer\n",
    "        long_out, surv_out = self.mmoe_ffn(x)\n",
    "        # Add & Norm: residual connection and LayerNorm for FFN output&#8203;:contentReference[oaicite:9]{index=9}\n",
    "        \n",
    "        # Return final representation and task outputs\n",
    "        return long_out, surv_out\n",
    "\n",
    "class TransformerDecoderMMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder with MMoE (single-layer decoder).\n",
    "    Wraps Decoder_Layer to produce multi-task outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, hidden_dim: int, num_experts: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimensionality (must match encoder output).\n",
    "            nhead: Number of attention heads.\n",
    "            hidden_dim: Hidden size for expert MLPs.\n",
    "            num_experts: Number of experts in the mixture.\n",
    "            dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderMMoE, self).__init__()\n",
    "        self.layer = Decoder_Layer2(d_model, nhead, hidden_dim, num_experts, dropout)\n",
    "    \n",
    "    def forward(self, encoder_out: torch.Tensor, attn_mask: torch.Tensor = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_out: Tensor [B, S, d_model] – output from the encoder (input to decode).\n",
    "            attn_mask: Optional mask for attention (same format as in Decoder_Layer).\n",
    "        Returns:\n",
    "            out: Tensor [B, S, d_model] – decoder output representation.\n",
    "            long_out: Tensor [B, S, 3] – longitudinal task predictions per token.\n",
    "            surv_out: Tensor [B, S, 1] – survival task predictions per token.\n",
    "        \"\"\"\n",
    "        long_out, surv_out = self.layer(encoder_out, attn_mask)\n",
    "        return long_out, surv_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/shijimao/TransformerJM/Models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/shijimao/TransformerJM/Simulation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (get_tensors, get_mask, init_weights, get_std_opt)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (long_loss, surv_loss)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AUC, Brier, MSE)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Models'"
     ]
    }
   ],
   "source": [
    "Models\n",
    "\n",
    "from Simulation.data_simulation_base import simulate_JM_base\n",
    "n_sim = 1\n",
    "I = 1000\n",
    "obstime = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "landmark_times = [1,2,3,4,5]\n",
    "pred_windows = [1,2,3]\n",
    "scenario = \"none\" # [\"none\", \"interaction\", \"nonph\"]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_simulation_base import simulate_JM_base\n",
    "data_all = simulate_JM_base(I=I, obstime=obstime, opt=scenario, seed=n_sim)\n",
    "data = data_all[data_all.obstime <= data_all.time]\n",
    "\n",
    "## split train/test\n",
    "random_id = range(I) #np.random.permutation(range(I))\n",
    "train_id = random_id[0:int(0.7*I)]\n",
    "test_id = random_id[int(0.7*I):I]\n",
    "\n",
    "train_data = data[data[\"id\"].isin(train_id)]\n",
    "test_data = data[data[\"id\"].isin(test_id)]\n",
    "x1= train_data[['X1','X2']]\n",
    "y = train_data[['Y1','Y2','Y3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_long:\n",
    "        Number of longitudinal outcomes\n",
    "    d_base:\n",
    "        Number of baseline / time-independent covariates\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_long,\n",
    "                 d_base,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 num_decoder_layers,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(d_long + d_base, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model)\n",
    "            )\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(d_model,nhead,dropout)\n",
    "                                             for _ in range(num_decoder_layers)])\n",
    "        \n",
    "    def forward(self, long, base, mask, obs_time):\n",
    "        # Concatenate longitudinal and baseline data\n",
    "        x = torch.cat((long, base), dim=2)\n",
    "        \n",
    "        # Linear Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Positional EmbeddingTransformerDecoderMMoE\n",
    "\n",
    "        x = x + positional_encoding(\n",
    "            x.shape[0], x.shape[1], x.shape[2], obs_time)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        for layer in self.decoder_layers:\n",
    "            decoding = layer(x, x, mask)\n",
    "\n",
    "        return decoding\n",
    "\n",
    "'''\n",
    "class Decoder_p(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters_\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 num_decoder_layers,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(d_model,nhead,dropout)\n",
    "                                             for _ in range(num_decoder_layers)])\n",
    "        \n",
    "    def forward(self, q, kv, mask, pred_time):\n",
    "        # Positional Embedding\n",
    "        \n",
    "        q = q + positional_encoding(\n",
    "            q.shape[0], q.shape[1], q.shape[2], pred_time)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(q, kv,mask)\n",
    "\n",
    "        return x\n",
    "'''\n",
    "class TransformerDecoderMMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder with MMoE (single-layer decoder).\n",
    "    Wraps Decoder_Layer to produce multi-task outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, hidden_dim: int, num_experts: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimensionality (must match encoder output).\n",
    "            nhead: Number of attention heads.\n",
    "            hidden_dim: Hidden size for expert MLPs.\n",
    "            num_experts: Number of experts in the mixture.\n",
    "            dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = Decoder_Layer2(d_model, nhead, hidden_dim, num_experts, dropout)\n",
    "    \n",
    "    def forward(self, encoder_out: torch.Tensor, attn_mask: torch.Tensor = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_out: Tensor [B, S, d_model] – output from the encoder (input to decode).\n",
    "            attn_mask: Optional mask for attention (same format as in Decoder_Layer).\n",
    "        Returns:\n",
    "            out: Tensor [B, S, d_model] – decoder output representation.\n",
    "            long_out: Tensor [B, S, 3] – longitudinal task predictions per token.\n",
    "            surv_out: Tensor [B, S, 1] – survival task predictions per token.\n",
    "        \"\"\"\n",
    "        long_out, surv_out = self.layer(encoder_out, attn_mask)\n",
    "        return long_out, surv_out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer1(nn.Module):\n",
    "    \"\"\"\n",
    "    An adaptation of the transformer model (Attention is All you Need)\n",
    "    for survival analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_long:\n",
    "        Number of longitudinal outcomes\n",
    "    d_base:\n",
    "        Number of baseline / time-independent covariates\n",
    "    d_model:\n",
    "        Dimension of the input vector (post embedding)\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    num_decoder_layers:\n",
    "        Number of decoder layers to stack\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_long,\n",
    "                 d_base,\n",
    "                 d_model = 32,\n",
    "                 nhead = 4,\n",
    "                 n_expert = 4,\n",
    "                 d_ff = 64,  \n",
    "                 num_decoder_layers = 3,\n",
    "                 dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.decoder = Decoder(d_long, d_base, d_model, nhead, num_decoder_layers, dropout)\n",
    "\n",
    "        #self.decoder_pred = Decoder_p(d_model, nhead, 1, dropout)\n",
    "        \n",
    "        self.long = nn.Sequential(\n",
    "            nn.Linear(d_model, d_long)\n",
    "        )\n",
    "        \n",
    "        self.surv = nn.Sequential(\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.decoder_mmoe = TransformerDecoderMMoE(d_model = d_model,nhead = nhead,hidden_dim = 64, num_experts = n_expert)\n",
    "\n",
    "    def forward(self, long, base, mask, obs_time, pred_time):        \n",
    "        # Decoder Layers\n",
    "        x = self.decoder(long, base, mask, obs_time)\n",
    "        \n",
    "        # Decoder Layer with prediction time embedding\n",
    "        \n",
    "        long,surv = self.decoder_mmoe(x, x, mask, pred_time)\n",
    "\n",
    "        return long, surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Each expert is a small feed-forward subnetwork.\n",
    "    Here, we map d_model -> d_ff -> d_model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, d_model]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MMoEHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared Expert Pool + 2 Gating Networks (one for each task).\n",
    "    Then each task does a final linear layer to get the actual prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_expert, d_long):\n",
    "        \"\"\"\n",
    "        d_model: dimension of transformer output\n",
    "        d_ff: hidden dimension inside each expert\n",
    "        n_expert: number of experts\n",
    "        d_long: dimension of the longitudinal output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create shared experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff) for _ in range(n_expert)\n",
    "        ])\n",
    "        # Each task has its own gating\n",
    "        self.gate_long = nn.Linear(d_model, n_expert)\n",
    "        self.gate_surv = nn.Linear(d_model, n_expert)\n",
    "\n",
    "        # Final output layers for each task\n",
    "        self.long_out = nn.Linear(d_model, d_long)  # e.g. predict d_long dims\n",
    "        self.surv_out = nn.Linear(d_model, 1)       # e.g. 1-dim survival logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, d_model] from the Transformer.\n",
    "        Returns:\n",
    "          long_pred: [B, T, d_long]\n",
    "          surv_pred: [B, T, 1]\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # 1) Compute each expert's output\n",
    "        #    We'll stack them: shape will be [B, T, n_expert, d_model]\n",
    "        expert_outs = []\n",
    "        for expert in self.experts:\n",
    "            e_out = expert(x)             # [B, T, d_model]\n",
    "            expert_outs.append(e_out)\n",
    "        # Stack along new dim=2 => (n_expert)\n",
    "        # result: [B, T, n_expert, d_model]\n",
    "        expert_outs = torch.stack(expert_outs, dim=2)\n",
    "\n",
    "        # 2) Gating for longitudinal\n",
    "        gate_logits_long = self.gate_long(x)              # [B, T, n_expert]\n",
    "        gate_weights_long = F.softmax(gate_logits_long, dim=-1)  # soft gating\n",
    "        # expand so we can multiply\n",
    "        gate_weights_long = gate_weights_long.unsqueeze(-1)       # [B, T, n_expert, 1]\n",
    "\n",
    "        # Weighted sum over the expert dimension\n",
    "        # shape => [B, T, d_model]\n",
    "        long_combined = (expert_outs * gate_weights_long).sum(dim=2)\n",
    "\n",
    "        # 3) Gating for survival\n",
    "        gate_logits_surv = self.gate_surv(x)              # [B, T, n_expert]\n",
    "        gate_weights_surv = F.softmax(gate_logits_surv, dim=-1)\n",
    "        gate_weights_surv = gate_weights_surv.unsqueeze(-1)       # [B, T, n_expert, 1]\n",
    "\n",
    "        surv_combined = (expert_outs * gate_weights_surv).sum(dim=2)\n",
    "\n",
    "        # 4) Final linear heads for each task\n",
    "        long_pred = self.long_out(long_combined)    # [B, T, d_long]\n",
    "        surv_logit = self.surv_out(surv_combined)    # [B, T, 1]\n",
    "        surv_pred = torch.sigmoid(surv_logit)        # or however you interpret survival\n",
    "\n",
    "        return long_pred, surv_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector\n",
    "    nhead:\n",
    "        Number of heads\n",
    "    dropout:\n",
    "        The dropout value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.Attention = MultiHeadAttention(d_model, nhead)\n",
    "                \n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(d_model,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,d_model),\n",
    "            nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        self.layerNorm1 = nn.LayerNorm(d_model)\n",
    "        self.layerNorm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q, kv, mask):\n",
    "        \n",
    "        # Attention\n",
    "        residual = q\n",
    "        x = self.Attention(query=q, key=kv, value=kv, mask = mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layerNorm1(x + residual)\n",
    "        \n",
    "        # Feed Forward\n",
    "        residual = x\n",
    "        x = self.feedForward(x)\n",
    "        x = self.layerNorm2(x + residual)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def positional_encoding(batch_size, length, d_model, obs_time):\n",
    "    \"\"\"\n",
    "    Positional Encoding for each visit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size:\n",
    "        Number of subjects in batch\n",
    "    length:\n",
    "        Number of visits\n",
    "    d_model:\n",
    "        Dimension of the model vector\n",
    "    obs_time:\n",
    "        Observed/recorded time of each visit\n",
    "    \"\"\"\n",
    "    PE = torch.zeros((batch_size, length, d_model)).to('cuda')\n",
    "    if obs_time.ndim == 0:\n",
    "        obs_time = obs_time.repeat(batch_size).unsqueeze(1)\n",
    "    elif obs_time.ndim == 1:\n",
    "        obs_time = obs_time.repeat(batch_size,1)\n",
    "    obs_time = obs_time.to('cuda')\n",
    "    pow0 = torch.pow(10000, torch.arange(0, d_model, 2, dtype=torch.float32)/d_model).to('cuda')\n",
    "\n",
    "    PE[:, :, 0::2] = torch.sin(torch.einsum('ij,k->ijk', obs_time, pow0))\n",
    "    pow1 = torch.pow(10000, torch.arange(1, d_model, 2, dtype=torch.float32)/d_model).to('cuda')\n",
    "    PE[:, :, 1::2] = torch.cos(torch.einsum('ij,k->ijk', obs_time, pow1))\n",
    "\n",
    "    return PE\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // nhead\n",
    "        self.nhead = nhead\n",
    "        \n",
    "        assert (\n",
    "            d_model % nhead == 0\n",
    "        ), \"Embedding size (d_model) needs to be divisible by number of heads\"\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def attention(self, query, key, value, d_k, mask = None, dropout=None):\n",
    "    \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) /  np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).to('cuda')\n",
    "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "            \n",
    "        output = torch.matmul(scores, value)\n",
    "        return output\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        I = query.shape[0]\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        query = self.q_linear(query).view(I, -1, self.nhead, self.d_k)\n",
    "        key = self.k_linear(key).view(I, -1, self.nhead, self.d_k)\n",
    "        value = self.v_linear(value).view(I, -1, self.nhead, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions I * nheanum_experts\n",
    "\n",
    "        # calculate attention\n",
    "        scores = self.attention(query, key, value, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(I, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MMoE_FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-gate Mixture-of-Experts Feed-Forward Network.\n",
    "    - Contains a shared pool of expert FFNs (each maps d_model -> hidden_dim -> d_model).\n",
    "    - Two task-specific gating networks (longitudinal and survival) produce softmax weights over experts.\n",
    "    - Combines expert outputs per task and passes them through task-specific linear heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, hidden_dim, num_experts):\n",
    "        super(MMoE_FFN, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Shared pool of expert networks (each expert is an FFN: Linear(d_model->hidden_dim) -> ReLU -> Linear(hidden_dim->d_model))\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, d_model)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Task-specific gating layers (one for each task) that output a weight for each expert\n",
    "        self.gate_long = nn.Linear(d_model, num_experts)  # Longitudinal task gate\n",
    "        self.gate_surv = nn.Linear(d_model, num_experts)  # Survival task gate\n",
    "        # Task-specific tower output layers\n",
    "        self.longitudinal_head = nn.Linear(d_model, 3)  # Outputs 3 features for longitudinal task\n",
    "        self.survival_head   = nn.Linear(d_model, 1)    # Outputs 1 feature for survival task\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Input tensor of shape [B, S, d_model].\n",
    "        :return: (long_out, surv_out, combined_output):\n",
    "                 - long_out: Longitudinal task output of shape [B, S, 3]\n",
    "                 - surv_out: Survival task output of shape [B, S, 1]\n",
    "        \"\"\"\n",
    "        B, S, D = x.size()\n",
    "        # Flatten batch and sequence dims for processing through experts and gates\n",
    "        x_flat = x.view(B * S, D)  # shape [B*S, d_model]\n",
    "        # Compute each expert's output for all positions (shared across tasks)\n",
    "        expert_outputs = []  # will collect outputs of shape [B*S, d_model] from each expert\n",
    "        for expert in self.experts:\n",
    "            expert_out = expert(x_flat)               # [B*S, d_model] output from this expert\n",
    "            expert_outputs.append(expert_out)\n",
    "        # Stack expert outputs into a single tensor of shape [B*S, num_experts, d_model]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # dim=1 indexes the expert\n",
    "        # Compute gating logits for each task and apply softmax to get mixture weights (one weight per expert)\n",
    "        gate_long_logits = self.gate_long(x_flat)  # [B*S, num_experts] (logits for longitudinal task experts)\n",
    "        gate_surv_logits = self.gate_surv(x_flat)  # [B*S, num_experts] (logits for survival task experts)\n",
    "        gate_long = F.softmax(gate_long_logits, dim=-1)  # [B*S, num_experts] weights for each expert (longitudinal)\n",
    "        gate_surv = F.softmax(gate_surv_logits, dim=-1)  # [B*S, num_experts] weights for each expert (survival)\n",
    "        # Reshape weights for broadcasting: [B*S, num_experts] -> [B*S, num_experts, 1]\n",
    "        gate_long = gate_long.unsqueeze(-1)\n",
    "        gate_surv = gate_surv.unsqueeze(-1)\n",
    "        # Compute weighted sum of expert outputs for each task using the gate weights\n",
    "        long_combined_flat = torch.sum(expert_outputs * gate_long, dim=1)  # [B*S, d_model] combined output for longitudinal task\n",
    "        surv_combined_flat = torch.sum(expert_outputs * gate_surv, dim=1)  # [B*S, d_model] combined output for survival task\n",
    "        # Reshape combined outputs back to [B, S, d_model]\n",
    "        long_combined = long_combined_flat.view(B, S, D)\n",
    "        surv_combined = surv_combined_flat.view(B, S, D)\n",
    "        # Compute final task-specific outputs via the tower heads\n",
    "        long_out = self.longitudinal_head(long_combined)  # [B, S, 3] longitudinal task output\n",
    "        surv_out = self.survival_head(surv_combined)      # [B, S, 1] survival task output\n",
    "    \n",
    "        return long_out, surv_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bd55910b8d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Simulation.data_simulation_base import simulate_JM_base\n",
    "n_sim = 1\n",
    "I = 1000\n",
    "obstime = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "landmark_times = [1,2,3,4,5]\n",
    "pred_windows = [1,2,3]\n",
    "scenario = \"none\" # [\"none\", \"interaction\", \"nonph\"]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_simulation_base import simulate_JM_base\n",
    "data_all = simulate_JM_base(I=I, obstime=obstime, opt=scenario, seed=n_sim)\n",
    "data = data_all[data_all.obstime <= data_all.time]\n",
    "\n",
    "## split train/test\n",
    "random_id = range(I) #np.random.permutation(range(I))\n",
    "train_id = random_id[0:int(0.7*I)]\n",
    "test_id = random_id[int(0.7*I):I]\n",
    "\n",
    "train_data = data[data[\"id\"].isin(train_id)]\n",
    "test_data = data[data[\"id\"].isin(test_id)]\n",
    "x1= train_data[['X1','X2']]\n",
    "y = train_data[['Y1','Y2','Y3']]\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"/home/shijimao/TransformerJM/Models\")\n",
    "sys.path.append(\"/home/shijimao/TransformerJM/Simulation\")\n",
    "from Models.Transformer.functions import (get_tensors, get_mask, init_weights, get_std_opt)\n",
    "from Models.Transformer.loss import (long_loss, surv_loss)\n",
    "from Models.metrics import (AUC, Brier, MSE)\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m batch_mask_out \u001b[38;5;241m=\u001b[39m batch_mask[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     34\u001b[0m obs_time \u001b[38;5;241m=\u001b[39m obs_time\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m yhat_long, yhat_surv \u001b[38;5;241m=\u001b[39m model(batch_long_inp, batch_base, batch_mask_inp,\n\u001b[1;32m     36\u001b[0m                 obs_time \u001b[38;5;241m=\u001b[39m obs_time[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), pred_time \u001b[38;5;241m=\u001b[39m obs_time[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     37\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m long_loss(yhat_long, batch_long_out, batch_mask_out)\n\u001b[1;32m     38\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m surv_loss(yhat_surv, batch_mask, batch_e)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 180\u001b[0m, in \u001b[0;36mTransformer1.forward\u001b[0;34m(self, long, base, mask, obs_time, pred_time)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, long, base, mask, obs_time, pred_time):        \n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Decoder Layers\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(long, base, mask, obs_time)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Decoder Layer with prediction time embedding\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     long,surv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_mmoe(x, x, mask, pred_time)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, long, base, mask, obs_time)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Decoder Layers\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m---> 61\u001b[0m     decoding \u001b[38;5;241m=\u001b[39m layer(x, x, mask)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoding\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mDecoder_Layer.forward\u001b[0;34m(self, q, kv, mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, kv, mask):\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Attention\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     residual \u001b[38;5;241m=\u001b[39m q\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAttention(query\u001b[38;5;241m=\u001b[39mq, key\u001b[38;5;241m=\u001b[39mkv, value\u001b[38;5;241m=\u001b[39mkv, mask \u001b[38;5;241m=\u001b[39m mask)\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerNorm1(x \u001b[38;5;241m+\u001b[39m residual)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stat293/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 123\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    118\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear(value)\u001b[38;5;241m.\u001b[39mview(I, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnhead, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# transpose to get dimensions I * nheanum_experts\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# calculate attention\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k, mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# concatenate heads and put through final linear layer\u001b[39;00m\n\u001b[1;32m    125\u001b[0m concat \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\\\n\u001b[1;32m    126\u001b[0m \u001b[38;5;241m.\u001b[39mview(I, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n",
      "Cell \u001b[0;32mIn[6], line 103\u001b[0m, in \u001b[0;36mMultiHeadAttention.attention\u001b[0;34m(self, query, key, value, d_k, mask, dropout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    104\u001b[0m scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = Transformer1(d_long=3, d_base=2, d_model=32, nhead=4,\n",
    "                    num_decoder_layers=7)\n",
    "model.to('cuda')\n",
    "model.apply(init_weights)\n",
    "model = model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_std_opt(optimizer, d_model=32, warmup_steps=200, factor=0.2)\n",
    "n_epoch = 50\n",
    "batch_size = 32\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")    \n",
    "    \n",
    "loss_values = []\n",
    "loss_test = []\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda')\n",
    "        batch_long_out = batch_long[:,1:,:].to('cuda')\n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time = obs_time[:,:-1].to('cuda'), pred_time = obs_time[:,1:].to('cuda'))\n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "        loss1_list.append(loss1.tolist())\n",
    "        loss2_list.append(loss2.tolist())\n",
    "    loss_values.append(running_loss.tolist())\n",
    "plt.plot((loss_values-np.min(loss_values))/(np.max(loss_values)-np.min(loss_values)), 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat293",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
