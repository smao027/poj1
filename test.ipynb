{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  visit  obstime  predtime  time  event         Y1        Y2        Y3  \\\n",
      "0   0      0        0         0     9   True  11.943728 -3.032593  2.760192   \n",
      "1   0      1        1         1     9   True  12.255357 -5.431790  4.225383   \n",
      "2   0      2        2         2     9   True  12.491947 -6.953460  2.854653   \n",
      "3   0      3        3         3     9   True  16.406431 -8.508030  4.766191   \n",
      "4   0      4        4         4     9   True  16.632347 -9.813989  5.816555   \n",
      "\n",
      "    X1        X2    pred_Y1   pred_Y2   pred_Y3      true  \n",
      "0  1.0  0.680195  11.943728 -3.032593  2.760192  1.000000  \n",
      "1  1.0  0.680195  12.255357 -5.431790  4.225383  0.999397  \n",
      "2  1.0  0.680195  12.491947 -6.953460  2.854653  0.998135  \n",
      "3  1.0  0.680195  16.406431 -8.508030  4.766191  0.995494  \n",
      "4  1.0  0.680195  16.632347 -9.813989  5.816555  0.989983  \n",
      "Number of unique IDs: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickle file back into a DataFrame\n",
    "data = pd.read_pickle('data/simulated_data.pkl')\n",
    "\n",
    "# Check the loaded data\n",
    "print(data.head())\n",
    "\n",
    "unique_id_count = data['id'].nunique()\n",
    "print(f\"Number of unique IDs: {unique_id_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "n_epoch = 50\n",
    "batch_size = 32\n",
    "    \n",
    "    \n",
    "loss_values = []\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    train_id = np.random.permutation(train_id)\n",
    "    for batch in range(0, len(train_id), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        indices = train_id[batch:batch+batch_size]\n",
    "        batch_data = train_data[train_data[\"id\"].isin(indices)]\n",
    "            \n",
    "        batch_long, batch_base, batch_mask, batch_e, batch_t, obs_time = get_tensors(batch_data.copy())\n",
    "        batch_long_inp = batch_long[:,:-1,:].to('cuda')\n",
    "        batch_long_out = batch_long[:,1:,:].to('cuda')\n",
    "        batch_base = batch_base[:,:-1,:].to('cuda')\n",
    "        batch_mask_inp = get_mask(batch_mask[:,:-1]).to('cuda')\n",
    "        batch_mask_out = batch_mask[:,1:].unsqueeze(2).to('cuda') \n",
    "        obs_time = obs_time.to('cuda')\n",
    "        yhat_long, yhat_surv = model(batch_long_inp, batch_base, batch_mask_inp,\n",
    "                        obs_time[:,:-1].to('cuda'), obs_time[:,1:].to('cuda'))\n",
    "        \n",
    "        loss1 = long_loss(yhat_long, batch_long_out, batch_mask_out)\n",
    "        loss2 = surv_loss(yhat_surv, batch_mask, batch_e)\n",
    "        \n",
    "        #loss = loss1 + loss2\n",
    "        loss = multi_task_loss(loss1, loss2)\n",
    "        \n",
    "        loss.backward()\n",
    "        scheduler.step()\n",
    "        running_loss += loss\n",
    "        loss1_list.append(loss1.tolist())\n",
    "        loss2_list.append(loss2.tolist())\n",
    "    loss_values.append(running_loss.tolist())\n",
    "plt.plot((loss_values-np.min(loss_values))/(np.max(loss_values)-np.min(loss_values)), 'b-')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat293",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
